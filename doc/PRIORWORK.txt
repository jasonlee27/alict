1. MT-NLP
Link: https://dl.acm.org/doi/abs/10.5555/3491440.3491504
MT-NLP applies a simple expansion that only adds an adjective word
before noun. Many adversarial generation techniques also rely on word
substitution, but some of them insert a word.

2. Astraea
Link: https://arxiv.org/abs/2010.02542
Astraea replaces words in an original sentence with other substitution
based on their pre-defined input grammar until it finds fairness
violation. It is limted to only rely on replacing noun words to test
the model fairness without changing the sentence structure.

3. Ogma
Link: https://arxiv.org/abs/1902.10027
Ogma replace words in an original sentence with other words with the
same tag of part-of-speech (POS), without changing the sentence
structure.

4. Textattack
Link: https://arxiv.org/abs/2005.05909
5. SememePSO (Word-level Textual Adversarial Attacking as Combinatorial
Optimization)
Link: https://arxiv.org/abs/1910.12196
6. Generating Natural Language Adversarial Examples
Link: https://arxiv.org/abs/1804.07998
7. BERT-ATTACK: Adversarial Attack Against BERT Using BERT
Link: https://arxiv.org/abs/2004.09984
This approach relies on substituting random words in an input sentence
with different words without changing the sentence structure. It focus
on attacking the model under test, but not differentiating if the
perturbed sentence attack the model by its adversarial properties or
by its context change from the input sentence.


==========
1. Checklist (Marco Tulio Ribeiro et al)
Link: https://arxiv.org/pdf/2005.04118.pdf
TASK: Sentiment analysis, duplicate question detection and machine
comprehension
IDEA: The checklist is the tool for comprehensive behavioral testing
of NLP models. It evaluates user-defined natural language capabilities
such as robustness, NER and negation etc. For each capability, test
cases are generated over 3 test types: Minimum functionality test,
Invariance test and Directional expectation test. In addition, the
test cases and perturbations can often be generalized into a template
and be generated from the template (e.g. I {negation} {pos_verb} the
{thing}). for each placeholder, checklist provides users with an
abstraction where they mask part of a template and get masked language
model (RoBERTa) suggestions for fill-ins. the RoBERTa suggestions can
be combined with WordNet categories (synonyms, antonyms, etc). Also,
checklist also provide additional common fill-ins for
general-purpose categories, such as Named Entities (common male and
female first/last names, cities, countries) and protected group
adjectives (nationalities, religions, gender and sexuality, etc)


2. Testing Machine Translation via Referential Transparency
(Pinjia He et al)
Link: https://pinjiahe.github.io/files/pdf/research/ICSE21.pdf
TASK: NMT
IDEA: The key insight is that a referentially transparent input is a
piece of text that should have similar translations when used in
different contexts. (1) identify noun phrases using a constituency
parser to collect a list of RTIs. (2) generates pairs in source
language by pairing an RTI with the full text in which it was found
and with all the containing RTIs from the same sentence. (3) is to
input RTI pairs to the machine translation software under test and
collect their translations. (4) translated pairs from (3) are checked
for RTI similarity in ordeer to detect translation errors.

3. Structure-Invariant Testing for Machine Translation (Pinjia He et al)
Link: https://pinjiahe.github.io/files/pdf/research/ICSE20.pdf
TASK: NMT
IDEA: The key insight is that similar source sentences (e.g. sentences
that differ by a single word) typically have translation results of
similar sentence structures. (1) SIT generatea a list of its similar
sentences by modifying a single word in the source sentences via NLP
techniques (i.e. BERT). (2) feeds all the sentences to the software
under test to ontain their translations. (3) uses specialized data
structures (i.e. constituency parse tree and dependency parse tree) to
represent the syntax structure of each of the translated
sentences. (4) compares the structures of the translated sentences.

4. Machine Translation Testing via Pathological Invariance (Shashij
Gupta et al)
Link: https://pinjiahe.github.io/files/pdf/research/ESECFSE20.pdf
TASK: NMT
IDEA: The key insight is that sentences with different meanings should
not have the same translation. (1) generate structually similar
sentences that have a different meaning as input for PatInv by
replacing a word and removing a word or phrase. (2) filter by
syntactic (constituency structure) and semantic information (synonyms
and sentence embeddings) (3) collects target sentences from the
generated sentences from (2). (4) detect translation errors.

5. Polyjuice (Tognshuang Wu et al)
Link: https://homes.cs.washington.edu/~marcotcr/acl21_polyjuice.pdf
TASK: task agnostic, but evaluated on Sentiment analysis, NLI, SNLI
and Duplicate question detection
IDEA: The Polyjoice is a counterfactual generator that allows for
control over perturbation types and locations trained by finetuning
GPT-2 on multiple datasets of paired sentences.

6. SEA & SEAR (Marco Tulio Ribeiro et al)
Link: https://homes.cs.washington.edu/~marcotcr/acl18.pdf
TASK: Machine comprehension, VQA
IEDA: it generates semantically equivalent adversaies (SEAs),
semantic-presetving perturbations that induce changes in the model's
predictions. It also generalize these adversaries into semantically
equivalent adversariaal rules (SEARs). Given input sentence, the
perturbations are generated using neural machine translation and beam
search. semantical similarity between input and its perturbations are
measured by probabilities and predictions. Given the perturbations,
SEARs generates a set of rules and filters the rules by measuring
semantic equivalence, high adversary count and non-redundancy.
