TASKS

  - [JL] On measuring seed sentences between Checklist and S2LCt:
    1. fix the upper-bound issue. 2. Measure coverage on all test
    seeds generated by our approach and Checklist. 3. Measure coverage
    on sampled test seeds generated by our approach and Checklist
    (sample to the size of whichever approach that generated smaller
    number of test cases).

  + [JL] Performance issue: 1. Optimize the Editor instance to
    instantiate Bert once.

  + [JL] update word_suggest on sa task as hs task. when multiple
    masks in a sentence is introduced, eval_sug_words_by_req(?) would
    check each suggested words for each mask.
  
  - [JL] modify our s2lct approach overview figure (This word
    sentiment dataset has been used for both generation and
    expansion. We discussed last time that generation and expansion
    should use different set of domain knowledge rather than the same
    one.  Generation is more about the attributes (e.g., sentiment,
    hatefulness) of tokens that should appear in the
    sentence. Expansion is about the constraints that prevent certain
    tokens to be appeared in the expanded part of the sentence. And
    also you should make this figure to be general for all tasks
    rather than just sentimental analysis task)
    

==========

  + [JL] create search/transform specification for hatecheck
    capabilities.
    
  + [JL] add all the related works metioned by the reviewer and more
    on the NLP testing. Also emphasize the difference of our work and
    related work.
    
  + [JL] writing retraining steps and asking for confirmation from
    wei&yang and do the experiment.
    
  + [JL] retraining with epochs 5 to 10

  + [JL] write experimental setup

  + [JL] understand DENAS for our implementation

  + [JL] get the eval result of model retrained on one lc over every lcs 

  + [JL] get the self-bleu score number for checklist testcases and
    our testcases
    
  + [JL] retraining models with original dataset(SST-2) + original
    failed cases

  + [JL] answer the shiyi's feedback on paper

  + [JL] implementing logging for logging all printing into file

  + [JL] make requirement description table in the paper.

  + [JL] extend/generate running_example.pdf figure in the paper.

  + [JL] make slides for structure of paper (overview, what how we would
    like to say from our approach/experiment).

  + [JL] search related works (mutation based nn work fuzzing)

  + [JL] think about how to statistically interact selected sentence
    and grammar checker.

  + [JL] implement my PCFG idea for generating exp sentences.

  + [JL] manual study of generated study: 1. labels of our all
    generated(expanded) test cases.

  + [JL] for AST differation, we check if we have enough syntax
    expansions (do seperate experiment)

  + [JL] For constructing PCFG probability in reference, multiply the
    p(non_terminal or terminal) with p(a->b).
    
  + [JL] think about CFG active learning. Given a sentence and its
    grammar CFG, expand the grammar using active learning (e.g. the
    grammar checker)

  + [JL] Read Glade paper and ARVADA paper.

  + [JL] use checklist released testcases as seed dataset and generate
    expanded testcases.

  + [JL] send/ask for mapping test type descriptions in CHECKLIST
    testsuites and test types listed in CHECKLIST paper (sentiment
    analysis)    

  + [JL] add more Temporal test capabilities.

  + [JL] add more Temporal Negation test capabilities.

  + [JL] add more Temporal SRL test capabilities.

  + [JL] test sentiment analysis models with the other dataset than
    SST (dynasent dataset)

  + [JL] using the seed dataset and its augmented dataset, re-train
    the models and see the performance on the other dataset.

  + [JL] expand the requirements defined to other testing linguistic
    capabilities.

  + [JL] evaluate the sugested words based on defined requirements and
    their tag of pos extracted from CFG difference

  + [JL] add more test capabilities (e.g. Robustness and NER)

  + [JL] complete writing code for the requirement of "Replace neutral
    words with other neutral words"
  
  + [JL] retrain the NLP model with CHECKLIST generated test cases and
    test the model using the proposed methodology

  + [JL] generated test cases using the proposed methodology searched
    over the CHECKLIST generated test cases (modified code, but
    not yet run)

  + [JL] get the original test performance of the model tested.

  + [JL] get the testsuite in checklist githubpage and run the models
    with the testsuite and compare the performance between the
    checklist and our testsuite.

  + [JL] split the test cases into seed and expanded test cases and
    sese the model prediction and compare them between the seed and
    expanded cases. if it is different, then the expanded element
    change the label.

  + [JL] extract word suggestion from language model (e.g. BERT)

  + [JL] given searched inputs that meets requirements

  + [JL] extract requirements from checklist sentiment analysis
    evaluation descriptions and search and transform input from
    database that meets the extracted requirements.
  
  + [JL] using expanded grammar of seed input grammar, how can we restrict 
    randomly generated sentences to meaningful natural language sentences?

  + [JL] make a new sentence generator. compared with the naive nltk generator,
    it needs to keep the seed input word and perturb it by replacing existing 
    words in the seed input with synonyms and adding new component in reference
    cfg.

  + [JL] search and transform input from dataset that meets the
    requirements extracted from 1st description (Short sentences with
    neutral adj and nouns)
    
  + [JL] generate manual sentiment analysis requirements from
    description of linguistic capabilities from checklist paper.

  + [JL] for each capability for testing NLP task, how do we extract
    its requirements from the description and use them for finding the
    most relevant data instances in dtaset.
  
  + [JL] manually generate more fine grained word categories e.g. pos_verb, neg_verb.

  + [JL] generate probability distribution over production rule in
    reference CFG.

  + [JL] given selected production rule in seed input, rank candidate
    production rules using the probability distribution.

  + [JL] generate random sentences using diff_cfg.
  
  + [JL] generate cfg difference given input cfg driven from data.
  
  + [JL] generate random sentences using generated_grammar.
  
  + [JL] generate residual cfg grammar between global and input-driven cfg grammar.

  - [JL] make a diff cfg (diff_cfg) between cfg in
    _results/checklist/sentanal_mft_template_grammar.txt and treebank
    global grammar.
  
  + [JL] generate/write cfg from input sentences.
