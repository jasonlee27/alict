TASKS

  - [JL] Read Glade paper and ARVADA paper.

  - [JL] think about CFG active learning. Given a sentence and its
    grammar CFG, expand the grammar using active learning (e.g. the
    grammar checker)

  - [JL] manual study of generated study: 1. labels of our all
    generated(expanded) test cases.
  
==========

  + [JL] use checklist released testcases as seed dataset and generate
    expanded testcases.

  + [JL] send/ask for mapping test type descriptions in CHECKLIST
    testsuites and test types listed in CHECKLIST paper (sentiment
    analysis)    

  + [JL] add more Temporal test capabilities.

  + [JL] add more Temporal Negation test capabilities.

  + [JL] add more Temporal SRL test capabilities.

  + [JL] test sentiment analysis models with the other dataset than
    SST (dynasent dataset)

  + [JL] using the seed dataset and its augmented dataset, re-train
    the models and see the performance on the other dataset.

  + [JL] expand the requirements defined to other testing linguistic
    capabilities.

  + [JL] evaluate the sugested words based on defined requirements and
    their tag of pos extracted from CFG difference

  + [JL] add more test capabilities (e.g. Robustness and NER)

  + [JL] complete writing code for the requirement of "Replace neutral
    words with other neutral words"
  
  + [JL] retrain the NLP model with CHECKLIST generated test cases and
    test the model using the proposed methodology

  + [JL] generated test cases using the proposed methodology searched
    over the CHECKLIST generated test cases (modified code, but
    not yet run)

  + [JL] get the original test performance of the model tested.

  + [JL] get the testsuite in checklist githubpage and run the models
    with the testsuite and compare the performance between the
    checklist and our testsuite.

  + [JL] split the test cases into seed and expanded test cases and
    sese the model prediction and compare them between the seed and
    expanded cases. if it is different, then the expanded element
    change the label.

  + [JL] extract word suggestion from language model (e.g. BERT)

  + [JL] given searched inputs that meets requirements

  + [JL] extract requirements from checklist sentiment analysis
    evaluation descriptions and search and transform input from
    database that meets the extracted requirements.
  
  + [JL] using expanded grammar of seed input grammar, how can we restrict 
    randomly generated sentences to meaningful natural language sentences?

  + [JL] make a new sentence generator. compared with the naive nltk generator,
    it needs to keep the seed input word and perturb it by replacing existing 
    words in the seed input with synonyms and adding new component in reference
    cfg.

  + [JL] search and transform input from dataset that meets the
    requirements extracted from 1st description (Short sentences with
    neutral adj and nouns)
    
  + [JL] generate manual sentiment analysis requirements from
    description of linguistic capabilities from checklist paper.

  + [JL] for each capability for testing NLP task, how do we extract
    its requirements from the description and use them for finding the
    most relevant data instances in dtaset.
  
  + [JL] manually generate more fine grained word categories e.g. pos_verb, neg_verb.

  + [JL] generate probability distribution over production rule in
    reference CFG.

  + [JL] given selected production rule in seed input, rank candidate
    production rules using the probability distribution.

  + [JL] generate random sentences using diff_cfg.
  
  + [JL] generate cfg difference given input cfg driven from data.
  
  + [JL] generate random sentences using generated_grammar.
  
  + [JL] generate residual cfg grammar between global and input-driven cfg grammar.

  - [JL] make a diff cfg (diff_cfg) between cfg in
    _results/checklist/sentanal_mft_template_grammar.txt and treebank
    global grammar.
  
  + [JL] generate/write cfg from input sentences.
