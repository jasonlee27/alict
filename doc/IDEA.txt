Research questions:
1. how they get label for the generated examples?
2. how they generate semantically correct examples?
3. what other NL tasks than NLT, sentiment analysis they can be applied to?

IDEA1:
    Given the test input sentences, we can generate set of context-free 
    grammar(CFG) rules(L*). Also, researchers have worked on parsing 
    natural languages (NLs) and cfg from large NL corpora (L_global). 
    I think that test inputs can be more diverse when we expand their grammar 
    and the grammar can be expanded from exploring difference beteen L_global 
    and L*. Knowing the difference and what cfg component can be used for 
    the grammar expansion, we can generate more diverse input templates and 
    more diverse input sentences.

IDEA2:
    Followed by the IDEA1, we implment probabilistic context-free
    grammar (PCFG). The problem when we generate sentences using CFG
    is that it is very unlikely to have semantically meaningful
    sentences because CFG only takes structural correctness into
    account. Therefore, naive sentence generator using the CFG
    generates sentences with too many repetitive words or with no
    context. When we extract and leverage probabilities over
    production rules from dataset, the probabilities represents
    naturalness of input language in real world, and ranking and
    expanding grammars from the probability distribution will
    generates more natural languages. Thereby, first, we generates
    probability distribution from L_global. Second, when we sample
    a production rule in parse tree of seed input and extract
    candidate expanded production rules, we rank them by the
    probability distribution, and select the top-k scored
    rules. Third, we generates sentences from the seed input as
    following conditions: 1. The generated sentences should be not
    same as the seed input. 2. difference between the generated
    sentences and seed input should be minimized (e.g. BLEU
    score). 3. The generated sentence should be semantically high
    confidence from language model(LM) such as BERT.  
    
IDEA3:

    for each NLP task and its model, there are multiple capabilities
    to be tested. For example, "Short sentences with neutral
    adjectives and nouns." and "Negated negative should be positive or
    neutral" are two of capability descriptions in the Checklist and
    sematic analysis task. For each capability description, it
    describe the requirements/instructions for how the sentences and
    its label have to look like. For "Short sentences with neutral
    adjectives and nouns", it requires sentences to be short and have
    neutral adjectives and nouns. Then we can list the requirements
    as following:
        1. len(sentence)<10
        2. #{pos/neg} adjectives==0
        3. #nouns>0 and #adjectives>0
    Then, in dataset, we can select sentences that meet such
    requirements, and use them as templates by replacing their
    adjectives and nouns with others. Therefore, in order to get more
    comprehensive templates, we can find data instances that meets
    such requirements set from testing capability, and use them as a
    new templates. Based on the idea, the steps are as following:
    Given target NLP task, its dataset (it could be train/val/test
    dataset) and its testing capabilities,
        1. For each capability, we construct its requirements.
        2. for data in {train/val/test} task relevant dataset,
           we select all data instances that meets the requirements.
        3. For each selected data from 2., we generate templates by
           replacing adjectives/verb/nouns.
        4. For each selected data from 2., we can also expand its
           grammar and adding BERT suggested words.
       
