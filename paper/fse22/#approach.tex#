\section{Technique and implementation}
%% \label{sec:approach}

\begin{figure*}
  \centering
  \includegraphics[scale=0.5]{figs/overview.pdf}
  \vspace{-5pt}
  \caption{\OverallModelFigCaption}
  \vspace{-10pt}
\end{figure*}

\Model generates input \sents with the following phases illustrated in
\ref{fig:OverallModel}: 1. search phase searches seed \sents according
to its \req of \lc, 2. seed parsing phase parses the found seed \sents
and extract their \cfg, 3. reference parsing phase collects \pcfg from
large corpus, 4. production differentiation phase identifies
structural expansion candidates for input expansion, and 5.\sent
selection phase generates natural expanded \sent. In this section, we
provide more details on each phase.

\subsection{Search phase}
The search phase in \Model searches inputs in dataset and selects
subset of input \sents in the dataset that meets the \lc
\req. The idea behind this phase is that input distribution of
\lc is important to generate inputs relevant to \lc. \Lc explains
expected behaviors of NLP model on specific types of input and
output. The NLP model is evaluated on how much it performs on the
input and output. Thus, \lc introduces the constraints of the input
data. Input data from the constrained distribution are only qualified
to be used for evaluating the NLP model on the \lc.  In addition,
diversity in inputs is important to evaluate NLP models on the
\lc. Inputs that differ are more likely to cover the NLP model
behavior, and more coverage increases trustworthiness of the
evaluation. To generate inputs from same distribution on \lc and high
diversity of inputs, we estabilish \reqs of input and output
for each \lc, and find inputs that fulfil the \reqs. Given a
\lc, a \req consists of search \req, transform
\req and expansion \req. The search \req
describes features and functionalities that we seek to have in
inputs. \Model check each input if it satisfy the \req.

\begin{figure}[t]
  \centering
  \lstinputlisting[language=json-pretty]{code/requirement_sa1.json}
  \vspace{-10pt}
  \caption{\SearchRequirementExampleFigCaption}
  \vspace{-10pt}
\end{figure}

\begin{figure}[t]
  \centering
  \subfloat[][\TransformRequirementExampleSubFigCaption]{\lstinputlisting[language=json-pretty]{code/requirement_sa2.json}}
  \\
  \subfloat[][\TransformTemplateExampleSubFigCaption]{\lstinputlisting[language=python-pretty]{code/requirement_sa2.py}}
  \\
  \caption{\TransformRequirementExampleFigCaption}
  \vspace{-10pt}
\end{figure}

Figure~\ref{fig:SearchReqEx} shows \lc of \SareqExOne. To evaluate
this \lc, the input is required to be short and have only \neu \adjs,
\neu \nns. In addition, the label needs to be \neu. Therefore, all
short natural \sents with only \neu \adjs and \neu \nns are available
to evaluate NLP models. Next, transform \req explains how the input
and output needs to be tranfromed. Some \lc only accepts heavily
limited input distribution, and it is unlikely to be included in
searching dataset because of its high structural diversity, thus,
finding such \sents is costly. Therefore, our approach is to find
inputs by relaxing search requirement and transform the input to match
the target requirement of the \lc. In this work, the inputs are
transformed by word addition or perturbing the found inputs with \lc
dependent templates. The figure~\ref{fig:TransformReqEx} shows the
example of use of the template requirement. The \lc of \SareqExTwo in
the figure~\ref{fig:TransformReqSubEx} requires inputs to be the
negated \pstv \sents and the neutral expression in the middle. Rather
than searching \sents that match the input distribution of the \lc,
the \Model search \pstv and \neu inputs and combine them into negated
\pstv \sents. Figure~\ref{fig:TransformTempSubEx} illustrates template
for the \lc. According to the \lc, The value of ``sent1'' and
``sent2'' become each searched \neu and \pstv inputs respectively, and
the template completion generates new inputs that matches the target
\lc. In addition, the transformation of inputs also produce high
diversity in the inputs because of that from initially found
inputs. In this paper, we will denote the searched inputs in this
phase as seed inputs.

\subsection{Seed parsing phase}
To expands seed sentence and generate fluent and faithful sentence
used for evaluation, \Model studies structure of each seed input for
its expansion. To extract the structure, this phase takes each seed as
input, and the parse tree of the seed input is output.  In this phase,
\Model implements the Berkeley Neural
Parser~\cite{kitaev2018seedparser,kitaev2019seedparser}.

\subsection{Reference parsing phase}
We take a large scale corpus as reference for seed expansion. It is
motivated by that large corpus represents data distribution of real
world, and the discrepancy between seed and reference leads to
determine how the seed can be expanded within real world data
distribution. This phase builds \pcfg from the reference corpus. \cfg
is constructed by parsing sentences in the corpus and extracting
production rules. In addition, the probability of \cfg is estimated by
its frequency over corpus. The outpus of this phase is the constructed
\pcfg, and it is compared with seed parse trees. For our
implementation, we build the \pcfg from the Penn Treebank corpus dataset.

\subsection{production differentiation phase}
\input{code/prod_diff_pseudocode}

Given the seed parse tree and reference \pcfg, \Model suggests
structural expansion candidates.


\subsection{\Sent selection phase}

