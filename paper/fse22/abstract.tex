\begin{abstract}
  \Nlp (NLP) technique becomes one of the core techniques for
  developing text analytics applications.  For developing an NLP
  application, the application is required to achieve high reliability
  before it goes to market.  The trustworthiness of the prevalent NLP
  applications is obtained by measuring the accuracy of the
  applications on hold-out dataset.  However, evaluating NLP on
  testset does with hold-out accuracy is limited to show its quality
  because the held-out datasets are often not comprehensive.

  %% connection saying that why behavior testing is employed.

  While the
  behavioral testing over multiple general linguistic capabilities are
  employed, the testing relies on manually created test cases, and is
  still limited to measure its comprehensive performance for each
  linguistic capability. In this work, we introduce \Model, an NLP
  model testing methodology. Given a linguistic capability, the \Model
  finds relevant testcases to test the linguistic capability from
  existing datasets as seed inputs, generates sufficient number of new
  test cases by fuzzing the seed inputs based on their \cfg (CFG). We
  illustrate the usefulness of the \Model by showing input diversity
  and identifying critical failures in \sota models for NLP task. In
  our experiment, we show that the \Model generates {} more test cases
  with {} higher diversity, and finds {} more bugs.
  
\end{abstract}
