\section{Technique and implementation}
%% \label{sec:approach}

\begin{figure*}
  \centering
  \includegraphics[scale=0.5]{figs/overview.pdf}
  \vspace{-5pt}
  \caption{\OverallModelFigCaption}
  \vspace{-10pt}
\end{figure*}

\Model generates input \sents with the following phases illustrated in
\ref{fig:OverallModel}: 1. search phase searches seed \sents according
to its \req of \lc, 2. seed parsing phase parses the found seed \sents
and extract their \cfg, 3. reference parsing phase collects \pcfg from
large corpus, 4. production differentiation phase identifies
structural expansion candidates for input expansion, and 5.\sent
selection phase generates natural expanded \sent. In this section, we
provide more details on each phase.

\subsection{Search phase}
The search phase in \Model searches inputs in dataset and selects
subset of input \sents in the dataset that meets the \lc
\req. The idea behind this phase is that input distribution of
\lc is important to generate inputs relevant to \lc. \Lc explains
expected behaviors of NLP model on specific types of input and
output. The NLP model is evaluated on how much it performs on the
input and output. Thus, \lc introduces the constraints of the input
data. Input data from the constrained distribution are only qualified
to be used for evaluating the NLP model on the \lc.  In addition,
diversity in inputs is important to evaluate NLP models on the
\lc. Inputs that differ are more likely to cover the NLP model
behavior, and more coverage increases trustworthiness of the
evaluation. To generate inputs from same distribution on \lc and high
diversity of inputs, we estabilish \reqs of input and output
for each \lc, and find inputs that fulfil the \reqs. Given a
\lc, a \req consists of search \req, transform
\req and expansion \req. The search \req
describes features and functionalities that we seek to have in
inputs. \Model check each input if it satisfy the \req.

\begin{figure}[t]
  \centering
  \lstinputlisting[language=json-pretty]{code/requirement_sa1.json}
  \vspace{-10pt}
  \caption{\SearchRequirementExampleFigCaption}
  \vspace{-10pt}
\end{figure}

\begin{figure}[t]
  \centering
  \subfloat[][\TransformRequirementExampleSubFigCaption]{\lstinputlisting[language=json-pretty]{code/requirement_sa2.json}}
  \\
  \subfloat[][\TransformTemplateExampleSubFigCaption]{\lstinputlisting[language=python-pretty]{code/requirement_sa2.py}}
  \\
  \caption{\TransformRequirementExampleFigCaption}
  \vspace{-10pt}
\end{figure}

Figure~\ref{fig:SearchReqEx} shows \lc of \SareqExOne. To evaluate
this \lc, the input is required to be short and have only \neu \adjs,
\neu \nns. In addition, the label needs to be \neu. Therefore, all
short natural \sents with only \neu \adjs and \neu \nns are available
to evaluate NLP models. In this work, the sentiment of the words for
the search are classified based on the sentiment scores from
\Swn~\cite{baccianella2010sentiwordnet}, a publicly available English
sentiment lexicons.  It provides lexical sentiment scores and the
sentiment word labels are categorized by implementing the rules
in~\cite{mihaela2017sentiwordnetlabel}. Next, transform \req explains
how the input and output needs to be tranfromed. Some \lc only accepts
heavily limited input distribution, and it is unlikely to be included
in searching dataset because of its high structural diversity, thus,
finding such \sents is costly. Therefore, our approach is to find
inputs by relaxing search requirement and transform the input to match
the target requirement of the \lc. In this work, the inputs are
transformed by word addition or perturbing the found inputs with \lc
dependent templates. The figure~\ref{fig:TransformReqEx} shows the
example of use of the template requirement. The \lc of \SareqExTwo in
the figure~\ref{fig:TransformReqSubEx} requires inputs to be the
negated \pstv \sents and the neutral expression in the middle. Rather
than searching \sents that match the input distribution of the \lc,
the \Model search \pstv and \neu inputs and combine them into negated
\pstv \sents. Figure~\ref{fig:TransformTempSubEx} illustrates template
for the \lc. According to the \lc, The value of ``sent1'' and
``sent2'' become each searched \neu and \pstv inputs respectively, and
the template completion generates new inputs that matches the target
\lc. In addition, the transformation of inputs also produce high
diversity in the inputs because of that from initially found
inputs. In this paper, we will denote the searched inputs in this
phase as seed inputs.

\subsection{Seed parsing phase}
To expands seed sentence and generate fluent and faithful sentence
used for evaluation, \Model studies structure of each seed input for
its expansion. To extract the structure, this phase takes each seed as
input, and the parse tree of the seed input is output.  In this phase,
\Model implements the Berkeley Neural
Parser~\cite{kitaev2018seedparser,kitaev2019seedparser}.

\subsection{Reference parsing phase}
We take a large scale corpus as reference for seed expansion. It is
motivated by that large corpus represents data distribution of real
world, and the discrepancy between seed and reference leads to
determine how the seed can be expanded within real world data
distribution. This phase builds \pcfg from the reference corpus. \cfg
is constructed by parsing sentences in the corpus and extracting
\prodrs. In addition, the probability of \cfg is estimated by
its frequency over corpus. The outpus of this phase is the constructed
\pcfg, and it is compared with seed parse trees. For our
implementation, we build the \pcfg from the Penn Treebank corpus dataset.

\subsection{production differentiation phase}
\input{code/prod_diff_pseudocode}
\begin{figure}
  \centering
  \includegraphics[scale=0.33]{figs/expansion.pdf}
  \vspace{-20pt}
  \caption{\ExpansionExCaption}
  \vspace{-30pt}
\end{figure}

Given the seed parse tree and reference \pcfg, production
differentiation phase suggests structural expansion candidates on the
seed input. this phase aims to analyze which structural components and
where they can be added into the seed structure for its expansion. To
do so, we explore reference \prodrs comparing it with each \prodr used
in seed input. This results in the phase described in
Algorithm~\ref{code:ProdDiffAlg}. For each production rule in seed
inputs ($seed\_prod$), it searches production rules in reference
($ref\_prod$) which it has same non-terminal on the \lhs
($seed\_prod.lhs==ref\_prod.lhs$) and superset of \rhs of the seed
production rule ($seed\_prod.rhs \subset ref\_prod.rhs$).  As we
assume that the reference \cfg is built from real world data
distribution, the elements in the complement set ($ref\_prod.rhs -
seed\_prod.rhs$) become an expansion candidate which can be expanded
from the $seed\_prod.rhs$ found in real world. In addition, the
measure of how consistent the production rule is with the given seed
structure is given in its probability of the reference production rule
($ref\_prob$) multiplied by that of parents of $seed\_prod$. The
expansion candidate consists of terminal or nonterminal symbols. When
there is a phrase-level or clause-level nonterminal symbol, \eg noun
phrase, it needs to be expanded and replaced with word-level
nonterminal or terminal symbols to generate the expansion
candidate. The number of feasible replacement is unbounded because of
its high degree of freedom. Therefore, in this work, we focus on the
expansion candidate with only the word-level nonterminal or terminal
symbols for the effectiveness of \Model. Lastly, the expanded
component is replaced with the mask token for the next phase. The
example of the expansion is illustrated in
Figure~\ref{fig:ExpEx}. The ``NP->[DT]'' is queried into
reference, and ``NP->[DT,NNS]'' is identified as its expansion
candidate since the \rhs of ``NP->[DT,NNS]'' is superset of that of
``NP->[DT]''. the component of NNS is replaced with mask token in
sentence-level. Therefore, the ``Or both \{MASK\}.'' is suggested for
the next phase.

\subsection{\Sent expansion and validation phase}

Next, the masked tokens in the expanded masked sentences are estimated
in this phase. For the mask token prediction, we use the BERT
pretrained model. The BERT model suggests word for the mask token
according to its context around in sentence. The suggested word are
validated by three criteria. First, The tag of POS must be matched with
that suggested from the production differentiation phase. In the
example in the Figure~\ref{fig:ExpEx}, the mask token comes from the
structural component of NNS, plural noun. Therefore, the BERT
suggested words must be also tagged with the NNS. Accordingly, every
words of the NNS are only available. Second, \Model focuses on the \sa
task, and it assume that the suggested words must not change sentiment
of its input and must preserve its consistency of original sentiment
label. Therefore, we only accept the \neu words for the
expansion. Third, the expanded sentences with the BERT suggested words
must be appropriate for evaluating NLP models on the target \lc, and the
sentences must pass the \req of the \lc. In this work, the BERT
suggestions are validated by the three criteria.
