\vspace{-6pt}
\section{Related Work}

\MyPara{NLP Testing}
%
With the increasing use of NLP models, evaluation of NLP models is
becoming a more important issue. Apart from the accuracy-based testing
scheme, recent works have considered model robustness as an
aspect for evaluation. Belinkov and Bisk
\textit{\etal}~\cite{belinkov2018breaknmt} aimed to fail neural machine
translation model by intentionally introducing noise in the input
text. Pinjia
\textit{\etal}~\cite{pinjia2020structinvtestingnmt,pinjia2020testnmtrt}
measured the robustness by assuming syntactic and semantic relation
between input and output of neural machine translation model.  Ribeiro
\textit{\etal}~\cite{ribeiro2018sear} proposed an approach to
generalize semantically equivalent adversarial rules. In addition,
Rychalska \textit{\etal}~\cite{rychalska2019wildnlp} measured drops in
BLEU scores by corruption operation, and compared model robustness
based on the amount of the drops. Iyyer
\textit{\etal}~\cite{iyyer2018adversarial} introduced learning-based
model for adversarial data augmentation.

In addition to the
robustness on adversarial set, various other aspects of the NLP model have been
considered for the robustness evaluation. Prabhakaran
\textit{\etal}~\cite{prabhakaran2019fairness} developed an evaluation
framework to detect unintended societal bias in NLP models. Rottger
\textit{\etal}~\cite{rottger2020hatecheck} introduced a functional
test suite for hate speech detection in the NLP model.  Ribeiro
\textit{\etal}~\cite{ribeiro2019consistencyeval} measured logical
consistency of NLP model. These techniques evaluate the robustness of
the NLP model. However, we focused on the evaluation of model capability
over multiple perspectives and produce debugging information by comparing
seed and expanded test cases.

\MyPara{Linguistic Capability Evaluation}
%
Wang \textit{\etal}~\cite{wang2018glue, wang2019superglue} proposed
multiple diagnostic datasets to evaluate NLP models. These datasets
evaluate NLP model's ability to understand input sentence via natural
language inference problems. More recently, \Cklst proposed an evaluation
method of input-output behavior defined as \lcs. \Cklst generates
behavior-guided inputs for validating the
behaviors.~\cite{marcoACL2020checklist}. Unlike prior work that relies on manual
test case generation, we used structural information in text to
generate test cases automatically.

\MyPara{NLP Model Debugging}
%
Researchers have been explaining NLP model prediction and
analyzing it for debugging the model. Ribeiro
\textit{\etal}~\cite{ribeiroSG16lime} evaluated model prediction guided by human feedback providing relevance scores for words on the model prediction. 
Interactive error analysis~\cite{wu2019errudite} also has been proposed to evaluate
model robustness. Zylberajch~\cite{zylberajch2021hildif} used influence functions to generate model explanation, and it enables interactive debugging incorporating humans feedback
explanation.
Lertvittaya~\cite{lertvittayakumjorn2020find} proposed
an approach to understand behavior of text classifier model and improve the
model by disabling irrelevant hidden features. In this work, \tool
is useful for identifying the sources of model failure as shown in RQ4.
