We thank the reviewer for their comments, which we will strive to
address.

**(Review A&B) How does S^2LCT differ from MT-NLP, Astraea, Ogma and adversarial generation techniques?**

We would like to point out two fundamental differences. First, the
linguistic capability(LC)-based testing requires S^2LCT generating
test cases that belong to the specific LC and conform to the label
defined by the LC. To the best of my knowledge, *no prior automated
approach can generate test cases that satisfy this requirement*, while
this is built into S^2LCT’s design, specifically using LC
specification to generate seeds and validating the expanded
sentences. Second, S^2LCT’s syntax-based sentence expansion improves
the diversity of syntax (RQ1) by extending the sentence
structures. Astraea and Ogma replace words in an original sentence
with other words with the same tag of part-of-speech (POS), not
changing the sentence structure. MT-NLP both replaces words and
expands input structure, but its expansion only adds an adjective word
before noun. Adversarial generation techniques replace words with
another one to cause the model to make a mistake. S^2LCT’s expansion
is more general by comparing the syntax in the reference corpus
because more diverse syntax in the reference corpus provides more
expansion rules.

**(Review B&C) Limited applicability & generalizability.**

S^2LCT takes the LC specifications and the labeled search database as
inputs (Figure 2). Applying S^2LCT to other NLP tasks requires
changing these inputs. Since the submission of the paper, we have
applied S^2LCT to test hate speech detection models. Using the
functionalities defined in [4], we show that S^2LCT also generates
more diverse test cases to test three hate speech detection models
than the manual approach in [4] did. We attached the results as
following table and will add them to the paper.

| Linguistic capability   | #Seeds   | #Exps | Failure rate[%] | #Pass-to-Fail |
| :---                             | :---           | :---       | :---                   | :---                  |
| LC1: Expression of strong negative emotions (explicit) | 140 | 799 | BERT: 0.00, daELECTRA: 89.99, daBERT: 97.98 | BERT: 0, daELECTRA: 19, daBERT: 22 |
| LC2: Description using very negative attributes (explicit) | 140 | 2959 | BERT: 0.00, daELECTRA: 97.16, daBERT: 98.97 | BERT: 0, daELECTRA: 30, daBERT: 23 |
| LC3: Dehumanization (explicit) | 140 | 3124 | BERT: 0.00, daELECTRA: 98.35, daBERT: 98.65 | BERT: 0, daELECTRA: 11, daBERT: 18 |
| LC4: Implicit derogation | 140 | 5664 | BERT: 0.00, daELECTRA: 95.21, daBERT: 98.73 | BERT: 0, daELECTRA: 56, daBERT: 30 |

| LC5: Direct threat | 133 | 2689 | BERT: 0.00, daELECTRA: 97.45, daBERT: 98.16 | BERT: 0, daELECTRA: 0, daBERT: 9 |
| LC6: Threat as normative statement | 140 | 4163 | BERT: 0.00, daELECTRA: 97.56, daBERT: 99.02 | BERT: 0, daELECTRA: 12, daBERT: 1 |
| LC7: Hate expressed using slur | 805 | 17318 | BERT: 0.00, daELECTRA: 94.97, daBERT: 97.10 | BERT: 0, daELECTRA: 81, daBERT: 70 |
| LC8: Non-hateful use of slur | 395 | 7881 | BERT: 100.00, daELECTRA: 3.84, daBERT: 2.68 | BERT: 0, daELECTRA: 20, daBERT: 26 |
| LC9: Hate expressed using profanity | 1842 | 49743 | BERT: 0.00, daELECTRA: 96.42, daBERT: 97.23 | BERT: 0, daELECTRA: 188, daBERT: 164 |
| LC10: Non-Hateful use of profanity | 701 | 15761 | BERT: 100.00, daELECTRA: 3.29, daBERT: 3.33 | BERT: 0, daELECTRA: 51, daBERT: 58 |
| LC11: Hate expressed through reference in subsequent clauses | 11968 | 44890 | BERT: 10.52, daELECTRA: 85.79, daBERT: 85.55 | BERT: 0, daELECTRA: 1164, daBERT: 1169 |
| LC12: Hate expressed through reference in subsequent sentences | 11968 | 42840 | BERT: 10.92, daELECTRA: 85.24, daBERT: 84.21 | BERT: 0, daELECTRA: 1070, daBERT: 1726 |
| LC13: Hate expressed using negated positive statement | 23379 | 126742 | BERT: 0.00, daELECTRA: 96.47, daBERT: 97.10 | BERT: 0, daELECTRA: 2804, daBERT: 2290 |

**(Review A&C) How much effort is involved in specifying the rules?**

LCs in CHECKLIST are described in a sentence, which is ambiguous. It
is not possible to accurately tell if a sentence belongs to a
CHECKLIST’s LC. We believe our rules for seed generation are more
formal. We spent less than 10 minutes generating the specification for
each LC. We will elaborate this effort in the paper.

**(Review A) How sensible are the generated sentences by the approach?**

S^2LCT seed sentences were generated by applying the search rules on
the labeled search database which contains real-world sentences. The
transformation templates, as shown in Table 1, allow generating
sensible seed sentences as they are straightforward concatenation of
sentence parts. Expanded sentences are also sensible because they are
generated by using a semantic-aware word suggestion model. The word
suggestion model suggests words to expand based on the surrounding
context in the seed sentences.

**(Review A) The subjects only include BERT or some of its variants.**

S^2LCT is a model-agnostic approach, and it works on a variety of NLP
models. We use BERT and some of its variants because these
transformer-based models are one of the most advanced types of NLP
models, which were also used in [3]. We would appreciate it if the
reviewer can suggest other specific models to test.

**(Review B) Motivation for using linguistic capabilities to generate testing samples.**

LCs are linguistic patterns that appear frequently in the natural
language for a NLP task. LCs, which define relations between the input
and output, have shown to be useful to test “functionalities” of the
NLP models [3]. Evaluation based on LCs allows us to assess to what
extent NLP models understand the relations and measure the model
performance per LC. S^2LCT generates test cases that can be mapped to
the LCs, thereby S^2LCT enables automated, comprehensive functionality
testing of the NLP model.

**(Review B) Formal definition of linguistic capabilities.**

As discussed above, we define LC specifications which are more formal
than the past works did. The linguistic capabilities are task and
language specific. We believe the linguistic capabilities should be
defined together linguists and NLP model developers for each specific
task.

**(Review B) How does the proposed method help improve the model's performance after finding bugs?**

In Section VI, we show that a specific pattern in an expanded sentence
causes erroneous behavior in the model. More test cases using this
pattern can be generated as additional training data to improve the
model performance. Our approach enables the detection of such patterns
by analyzing the changes of the model behavior on the seed and
expanded sentences.

**(Review C) Fail rate for CHECKLIST.**

We have provided the failure rate in the following table. The order of
the LCs are same as one in table 2 in the paper. We will also add them
to the paper.

| Linguistic capability   | #test cases   | Failure rate [%]                                                              |
| :---                             | :---                 | :---                                                                                  |
| LC1                           | 1716             | BERT: 77.51, RoBERTa: 81.06, dstBERT: 96.79           |
| LC2                           | 8658             | BERT: 0.30, RoBERTa: 1.61, dstBERT: 1.44                 |
| LC3                           | 8000             | BERT: 21.00, RoBERTa: 10.36, dstBERT:31.65            |
| LC4                           | 6786             | BERT: 11.77, RoBERTa: 3.21, dstBERT: 10.82             |
| LC5                           | 2496             | BERT: 97.24, RoBERTa: 92.31, dstBERT: 98.16           |
| LC6                           | 2124             | BERT: 88.09, RoBERTa: 20.95, dstBERT: 100.00         |
| LC7                           | 1000             | BERT: 86.00, RoBERTa: 41.60, dstBERT: 86.50           |
| LC8                           | 8528             | BERT: 43.87, RoBERTa: 31.57, dstBERT: 41.45           |
| LC9                           | 9204             | BERT: 19.48, RoBERTa: 13.71, dstBERT: 16.92           |
| LC10                         | 7644             | BERT: 53.06, RoBERTa: 59.86, dstBERT: 84.25           |

**(Review C) Label consistency.**

The reviewer is right that the label inconsistency may affect the
evaluation of the models. We would like to note that most
inconsistency originated from the seed sentences (in Table III, for
almost all expanded sentences with the inconsistent label, its seed
has an inconsistent label). We manually checked all the pass-to-fail
cases in Table II (last column) and confirmed they all have consistent
labels.




==========

uniqueness of the test generation over lc
difference metamorphic testing than our work

ours: both label-consistency and LC-consistency
manually discovered label consistency

1. we generate new structure than other work

our work more generally preserves semantic context(capability part)
and grammatical correctness(syntactical part).

==========
Thank you for the comments.

__REVIEW_1&2

>Uniqueness of S^2LCT over prior work (MT-NLP, Astraea, Ogma and
 adversarial methods for NLP models)

S^2LCT is based on a grammar based test case generation method. The
uniqueness of the S^2LCT stems from the following properties: First,
S^2LCT generates new test cases with new and diverse input structures
given the grammar obtained from commonly used large corpus.  Compared
with the method, Astraea and Ogma generates input sentences by
replacing words in an original sentences with other words with same
tag of part-of-speech (POS) conforming to the grammar manually
constructed. In addition, MT-NLP both replaces words and expands input
structure, but the expansion only relies on a addition of adjective
word before noun. In our paper, the structural diversity is shown by
production rule coverage. Second, S^2LCT addresses consistency of
label and linguistic capability (LC). The Ogma does not aim to
preserve the semantic similarity, and it causes label inconsistency in
input generation. In addition, Astraea and MT-NLP manually preserves
label consistency by only relying on limited grammar attributes
(e.g. replacement of noun words). Compared with it, S^2LCT preserves
label and LC consistency by analyzing syntactic and semantic
appropriateness of expanded words in the input sentence. Besides,
S^2LCT generates test cases suitable to the LC and assess how the
model behaves on a specific linguistic characteristics in input
sentences. This method is different from the adversarial methods in a
sense that the adversarial approaches rely on generating error-causing
test cases, and they are limited to assess NLP models on multiple
linguistic characteristics.

>Motivation of using linguistic capabilities

Traditional train-valid-test split tests NLP models aggregately and
the test set reveals biases and fail to comprehensive testing the
models. Therefore, prior work (e.g. CHECKLIST) introduced multiple
task relevant LCs and relevant test cases. It enables assess NLP
models over the multiple LCs. However, it requires manual effort to
generate relevant test cases, and it results in obtaining limited
semantic and syntactic attributes. In addition, It requires
non-trivial manual efforts to define the metamorphic relations between
inputs and outputs because semantics of nature language can be greatly
changed even by a slight perturbation to the sentences. In this work,
we automates LC-relevant test cases generation using the same LCs
provided in CEHCKLIST.

__REVIEW_2&3

>Limited Applicability & Generalizability

We show the applicability and generalizability of the S^2LCT by
implementing it for other NLP task. In this work, we implement the
S^2LCT for hate speech detection task. For its experiment, we use
first 14 functionalities for the task introduced from the hatecheck
(https://arxiv.org/abs/2012.15606), and we use all seeds for its
expansion. Models under test for this experiment are
Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two,
DaNLP/da-electra-hatespeech-detection and
DaNLP/da-bert-hatespeech-detection. The experiment result over the
functionalities are added in the attachment. We will also add the
results in our final version.

__REVIEW_1

>How much effort involved in specifying the rules?

LCs in CHECKLIST are described in a sentence, thus it is not clear to
specify linguistic characteristics of interest and how the input and
output should be formed because of semantic and structure diversity in
natural language. Thus, we manually specify rule for seed generation
for each LCs in CHECKLIST to achieve the semantic and structure
diversity. In practice, we spent less than 10 min to generate each LC.

__REVIEW_2

>No discussion or results about repair strategy.

In the section VI of our paper, we show the effectiveness of S^2LCT
for finding root causes of bugs.

__REVIEW_3

>Significant effort in defining search rules and transformation

I agree that S^2LCT requires manual specification of search rules and
transformation.

>Fail Rate for CHECKLIST

We add the fail rates for CHECKLIST in the attachment.
We will also add the results in our final version.

>Label Consistency

During the manual study to measure label consistency, it is observed
that the 83% of label consistency of the seed sentences comes from
inconsistency between label and the raw sentences in the dataset, and
it results in the 84% of label consistency of expanded
sentences. Accordingly, we also empirically compare seed sentences and
their expanded sentences from the pass-to-fail cases in the table II
to check the label consistency of the expanded sentences. From the
comparison, it is observed that no expanded words change the sentiment
of the expanded sentences and the expanded words preserves the label
consistency.
