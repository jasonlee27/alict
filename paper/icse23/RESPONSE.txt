We thank the reviewers for their comments.

**(Review A&B) How does S^2LCT differ from MT-NLP, Astraea, Ogma and adversarial generation techniques?**

We would like to point out two fundamental differences. First, the
linguistic capability(LC)-based testing requires S^2LCT generating
test cases that belong to a specific LC and conform to the label
defined by the LC throughout the test generation process. To the best
of our knowledge, *no prior automated approach can generate test cases
that satisfy this requirement of LC-based testing*, while this is
built into S^2LCT’s design, specifically using LC specification to
generate seeds and validating the expanded sentences. Second, S^2LCT’s
syntax-based sentence expansion improves the diversity of syntax (RQ1)
by extending the sentence structures. Astraea and Ogma replace words
in an original sentence with other words with the same tag of
part-of-speech (POS), without changing the sentence structure. MT-NLP
applies a simple expansion that only adds an adjective word before
noun. Many adversarial generation techniques also rely on word
substitution, but some of them insert a word [5]. However, it is
limited to keep label consistency. S^2LCT’s expansion finds natural
variance of sentence structures in the reference corpus which lead to
diverse syntax in expansion rules. In addition, it accepts words that
do not influence semantics in seed sentences and preserves label
consistency in the generated test cases.

**(Review B&C) Limited applicability & generalizability.**

S^2LCT takes the LC specifications and the labeled search database as
inputs (Figure 2). Applying S^2LCT to other NLP tasks requires
changing these inputs. Actually we have applied S^2LCT to test hate
speech detection models (results were not reported in this submission
due to space limit). Using the functionalities defined in [4], we show
that S^2LCT also generates more diverse test cases to test three hate
speech detection models than the manual approach in [4] did. We
attached the results as the following table and will add them to the
final version of the paper.

| Linguistic capability   | #Seeds   | #Exps | Failure rate[%] | #Pass-to-Fail |
| :---                             | :---           | :---       | :---                   | :---                  |
| LC1: Expression of strong negative emotions (explicit) | 140 | 799 | BERT: 0.00, daELECTRA: 89.99, daBERT: 97.98 | BERT: 0, daELECTRA: 19, daBERT: 22 |
| LC2: Description using very negative attributes (explicit) | 140 | 2959 | BERT: 0.00, daELECTRA: 97.16, daBERT: 98.97 | BERT: 0, daELECTRA: 30, daBERT: 23 |
| LC3: Dehumanization (explicit) | 140 | 3124 | BERT: 0.00, daELECTRA: 98.35, daBERT: 98.65 | BERT: 0, daELECTRA: 11, daBERT: 18 |
| LC4: Implicit derogation | 140 | 5664 | BERT: 0.00, daELECTRA: 95.21, daBERT: 98.73 | BERT: 0, daELECTRA: 56, daBERT: 30 |

| LC5: Direct threat | 133 | 2689 | BERT: 0.00, daELECTRA: 97.45, daBERT: 98.16 | BERT: 0, daELECTRA: 0, daBERT: 9 |
| LC6: Threat as normative statement | 140 | 4163 | BERT: 0.00, daELECTRA: 97.56, daBERT: 99.02 | BERT: 0, daELECTRA: 12, daBERT: 1 |
| LC7: Hate expressed using slur | 805 | 17318 | BERT: 0.00, daELECTRA: 94.97, daBERT: 97.10 | BERT: 0, daELECTRA: 81, daBERT: 70 |
| LC8: Non-hateful use of slur | 395 | 7881 | BERT: 100.00, daELECTRA: 3.84, daBERT: 2.68 | BERT: 0, daELECTRA: 20, daBERT: 26 |
| LC9: Hate expressed using profanity | 1842 | 49743 | BERT: 0.00, daELECTRA: 96.42, daBERT: 97.23 | BERT: 0, daELECTRA: 188, daBERT: 164 |
| LC10: Non-Hateful use of profanity | 701 | 15761 | BERT: 100.00, daELECTRA: 3.29, daBERT: 3.33 | BERT: 0, daELECTRA: 51, daBERT: 58 |
| LC11: Hate expressed through reference in subsequent clauses | 11968 | 44890 | BERT: 10.52, daELECTRA: 85.79, daBERT: 85.55 | BERT: 0, daELECTRA: 1164, daBERT: 1169 |
| LC12: Hate expressed through reference in subsequent sentences | 11968 | 42840 | BERT: 10.92, daELECTRA: 85.24, daBERT: 84.21 | BERT: 0, daELECTRA: 1070, daBERT: 1726 |
| LC13: Hate expressed using negated positive statement | 23379 | 126742 | BERT: 0.00, daELECTRA: 96.47, daBERT: 97.10 | BERT: 0, daELECTRA: 2804, daBERT: 2290 |

**(Review A&C) How much effort is involved in specifying the rules?**

We spent less than 10 minutes generating the specification for each
LC. Note that LCs in CHECKLIST are described in a non-formal natural
language sentence, which is ambiguous. It is impossible to determine
automatically whether a sentence belongs to a CHECKLIST’s
LC. Comparatively, our rules for seed generation are formal and can be
automatically checked against. We will elaborate the rule-specifying
effort in the paper.

**(Review A) How sensible are the generated sentences by the approach?**

S^2LCT ensures the sensibility of generated sentences from both syntax
and semantic levels. From the syntax level, seed sentences were
generated by applying the search rules on the labeled search database
which contains real-world sentences. The transformation templates, as
shown in Table 1, allow generating sensible seed sentences as they are
straightforward concatenation of sentence parts. From the semantic
level, expanded sentences are also sensible because they are generated
by using a semantic-aware word suggestion model. The word suggestion
model suggests words to expand based on the surrounding context in the
seed sentences.

**(Review A) The subjects only include BERT or some of its variants.**

S^2LCT is a model-agnostic approach, and it works on a variety of NLP
models. We use BERT and some of its variants because these
transformer-based models are one of the most advanced types of NLP
models, which were also used in [3]. We would appreciate it if the
reviewer can suggest other specific models to test.

**(Review B) Motivation for using linguistic capabilities to generate testing samples.**

LCs are linguistic patterns that appear frequently in the natural
language for a NLP task. LCs, which define relations between the input
and output, have shown to be useful to test “functionalities” of the
NLP models [3]. Evaluation based on LCs allows us to assess to what
extent NLP models understand the relations and measure the model
performance per LC. S^2LCT generates test cases that can be mapped to
the LCs, thereby S^2LCT enables automated, comprehensive functionality
testing of the NLP model.

**(Review B) Formal definition of linguistic capabilities.**

As discussed above, we define LC specifications which are more formal
than the past works did. The linguistic capabilities are task and
language specific. We believe the linguistic capabilities should be
defined together with linguists and NLP model developers for each
specific task.

**(Review B) How does the proposed method help improve the model's performance after finding bugs?**

In Section VI, we show that a specific pattern in an expanded sentence
causes erroneous behavior in the model. More test cases using this
pattern can be generated as additional training data to improve the
model performance. Our approach enables the detection of such patterns
by analyzing the changes of the model behavior on the seed and
expanded sentences.

**(Review C) Fail rate for CHECKLIST.**

We have provided the failure rate in the following table. The order of
the LCs are the same as one in table 2 in the paper. We will also add
them to the paper.

| Linguistic capability   | #test cases   | Failure rate [%]                                                              |
| :---                             | :---                 | :---                                                                                  |
| LC1                           | 1716             | BERT: 77.51, RoBERTa: 81.06, dstBERT: 96.79           |
| LC2                           | 8658             | BERT: 0.30, RoBERTa: 1.61, dstBERT: 1.44                 |
| LC3                           | 8000             | BERT: 21.00, RoBERTa: 10.36, dstBERT:31.65            |
| LC4                           | 6786             | BERT: 11.77, RoBERTa: 3.21, dstBERT: 10.82             |
| LC5                           | 2496             | BERT: 97.24, RoBERTa: 92.31, dstBERT: 98.16           |
| LC6                           | 2124             | BERT: 88.09, RoBERTa: 20.95, dstBERT: 100.00         |
| LC7                           | 1000             | BERT: 86.00, RoBERTa: 41.60, dstBERT: 86.50           |
| LC8                           | 8528             | BERT: 43.87, RoBERTa: 31.57, dstBERT: 41.45           |
| LC9                           | 9204             | BERT: 19.48, RoBERTa: 13.71, dstBERT: 16.92           |
| LC10                         | 7644             | BERT: 53.06, RoBERTa: 59.86, dstBERT: 84.25           |

**(Review C) Label consistency.**

The reviewer is right that the label inconsistency may affect the
evaluation of the models. We would like to note that most
inconsistency originated from the seed sentences (in Table III, for
almost all expanded sentences with the inconsistent label, its seed
has an inconsistent label). We manually checked all the pass-to-fail
cases in Table II (last column) and confirmed they all have consistent
labels.
