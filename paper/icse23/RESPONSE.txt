Thank you for the comments.

__REVIEW_1

>Differences with prior grammar-based work (e.g., MT-NLP, Astraea,
 Ogma etc)

Our method is a grammar-based behavioral testing method for over
linguistic capabilities(LCs). On the other hand, to best of my
knowledge, the existing work (e.g. Ogma, Astraea and MT-NLP) focus on
testing fairness or generating adversarial inputs.  Second, our method
secure label consistency by only allowing neutral words from the set
of contextually suggested words while prior work does not garauntee
the label consistency as they only focus on input generation. For
example, Ogma and Astraea replace a word with a different word that
has same part-of-speech. the method is limited to secure the semantic
of input sentence, and it does not gaurantee same label as a
result. Third, ours generates new test cases with a variety of new
input structures while others rely on the original structure, new
input sentences are limited to the original structure based on their
manually generated grammar. Additionally, our work selects
grammatically and contextually natural words suggested from CFG
grammar and word suggestion model. In contrast, the existing work
randomly selects words.

>How much effort involved in specifying the rules?

LCs in CHECKLIST are described in a sentence, thus it is not clear to
specify linguistic characteristics of interest and how the input and
output should be formed because of semantic and structure diversity in
natural language. Thus, we manually specify rule for seed generation
for each LCs in CHECKLIST to achieve the semantic and structure
diversity. In practice, we spent less than 10 min to generate each LC.

>Introducing the linguistic capability...the loop framework.

Our work uses the same LCs provided in CHECKLIST. The LCs are
pre-defined manually for each task of interest.

__REVIEW_2

>Do not see a clear motivation of using linguistic capabilities.

Our work uses the same LCs provided in CEHCKLIST. They describe
specific metamorphic relations between linguistic patten in input
sentence and its output. Compared to the traditional train-valid-test
split to test NLP models aggregately, Use of the LCs is able to assess
NLP models over multiple linguistic characteristics in natural
language relevant to task. Therefore, it enables model behavior
testing finding erroneous patterns of models.

>Comparison of S^2LCT with NLP adversarial machine learning [2-5].

Extended from the previous response above, S^2LCT evaluates model
behavior under task-relevant LCs. S^2LCT generates test cases suitable
to given LC. Compared with out work, adversarial approaches focus on
generating error-causing test cases. It is limited to assess NLP
models over multiple linguistic characteristics.

>Generalizability of the proposed method seems limited.

S^2LCT is also implemented for hate speech detection task.  We will
add results of the task in our final version [JL] or appendix?.

>No discussion or results about repair strategy.

In the section VI of our paper, we show the effectiveness of S^2LCT
for finding root causes of bugs.

__REVIEW_3

>Limited Applicability & Generalizability

See response to REVIEW_2

>Fail Rate for CHECKLIST

[JL] do we add the numbers here or say that we will add that in the
appendix? or final version?

>label consistency results for CHECKLIST

[JL] do we add the numbers here or say that we will add that in the appendix?
