@String{AAAI = "AAAI Conference on Artificial Intelligence"}
@String{ACL = "Annual Meeting of the Association for Computational Linguistics"}
@String{AOSD = "International Conference on Aspect-Oriented Software Development"}
@String{ASE = "Automated Software Engineering"}
@String{ASETool = "Automated Software Engineering, Tool Demonstrations"}
@String{CADE = "International Conference on Automated Deduction"}
@String{CAV = "International Conference on Computer Aided Verification"}
@String{CICLing = "International Conference on Intelligent Text Processing and Computational Linguistics"}
@String{CICM = "International Conference on Intelligent Computer Mathematics"}
@String{CICMWIP = "Work in Progress at the Conference on Intelligent Computer Mathematics"}
@String{COLING = "International Conference on Computational Linguistics"}
@String{COQPL = "International Workshop on Coq for Programming Languages"}
@String{CPP = "Certified Programs and Proofs"}
@String{CSUR = "ACM Computing Surveys"}
@String{EACL = "Conference of the European Chapter of the Association for Computational Linguistics"}
@String{ECOOP = "European Conference on Object-Oriented Programming"}
@String{EMNLP = "Empirical Methods in Natural Language Processing"}
@String{ESEC/FSE = "Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering"}
@String{ESOP = "European Symposium on Programming"}
@String{FASE = "Fundamental Approaches to Software Engineering"}
@String{FM = "International Symposium on Formal Methods"}
@String{FSE = "International Symposium on the Foundations of Software Engineering"}
@String{FSENIER = "International Symposium on the Foundations of Software Engineering, NIER"}
@String{ICLR = "International Conference on Learning Representations"}
@String{ICML = "International Conference on Machine Learning"}
@String{ICPC = "International Conference on Program Comprehension"}
@String{ICSE = "International Conference on Software Engineering"}
@String{ICSME = "International Conference on Software Maintenance and Evolution"}
@String{ICST = "International Conference on Software Testing, Verification, and Validation"}
@String{IJCAI = "International Joint Conference on Artificial Intelligence"}
@String{IJCAR = "International Joint Conference on Automated Reasoning"}
@String{ISSTA = "International Symposium on Software Testing and Analysis"}
@String{ITP = "International Conference on Interactive Theorem Proving"}
@String{LPAR = "International Conference on Logic for Programming, Artificial Intelligence, and Reasoning"}
@String{MSR = "International Conference on Mining Software Repositories"}
@String{NAACL = "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}
@String{NeurIPS = "Annual Conference on Neural Information Processing Systems"}
@String{NIPS = "Annual Conference on Neural Information Processing Systems"}
@String{OOPSLA = "International Conference on Object-Oriented Programming, Systems, Languages, and Applications"}
@String{PLDI = "Conference on Programming Language Design and Implementation"}
@String{POPL = "Symposium on Principles of Programming Languages"}
@String{SANER = "International Conference on Software Analysis, Evolution and Reengineering"}
@String{SOOPPA = "Symposium on Object-Oriented Programming Emphasizing Practical Applications"}
@String{SOSP = "Symposium on Operating Systems Principles"}
@String{TPHOLs = "Theorem Proving in Higher Order Logics"}
@String{TYPES = "International Conference on Types for Proofs and Programs"}
@String{UITP = "International Workshop On User Interfaces for Theorem Provers"}
@String{WCRE = "Working Conference on Reverse Engineering"}

@article{huang2022aeon,
  title={AEON: A Method for Automatic Evaluation of NLP Test Cases},
  author={Huang, Jen-tse and Zhang, Jianping and Wang, Wenxuan and He, Pinjia and Su, Yuxin and Lyu, Michael R},
  journal={arXiv preprint arXiv:2205.06439},
  year={2022}
}

@article{gururangan2018annotation,
  title={Annotation artifacts in natural language inference data},
  author={Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel R and Smith, Noah A},
  journal={arXiv preprint arXiv:1803.02324},
  year={2018}
}

@article{geva2019we,
  title={Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets},
  author={Geva, Mor and Goldberg, Yoav and Berant, Jonathan},
  journal={arXiv preprint arXiv:1908.07898},
  year={2019}
}

@article{rottger2020hatecheck,
  title={Hatecheck: Functional tests for hate speech detection models},
  author={R{\"o}ttger, Paul and Vidgen, Bertram and Nguyen, Dong and Waseem, Zeerak and Margetts, Helen and Pierrehumbert, Janet B},
  journal={arXiv preprint arXiv:2012.15606},
  year={2020}
}

@InProceedings{recht2019imagenetbias,
  title = 	 {Do {I}mage{N}et Classifiers Generalize to {I}mage{N}et?},
  author =       {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5389--5400},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/recht19a/recht19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/recht19a.html},
  abstract = 	 {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly "harder" images than those found in the original test sets.}
}

@inproceedings{wu2019errudite,
    title = "{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",
    author = "Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1073",
    doi = "10.18653/v1/P19-1073",
    pages = "747--763",
    abstract = "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",
}

@inproceedings{marcoACL2020checklist,  
 author = {Marco Tulio Ribeiro and Tongshuang Wu and Carlos Guestrin and Sameer Singh},  
 title = {Beyond Accuracy: Behavioral Testing of NLP models with CheckList},  
 booktitle = {Association for Computational Linguistics (ACL)},  
 year = {2020}
}

@inproceedings{ribeiro2018sear,
    title = "Semantically Equivalent Adversarial Rules for Debugging {NLP} models",
    author = "Ribeiro, Marco Tulio  and
      Singh, Sameer  and
      Guestrin, Carlos",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1079",
    doi = "10.18653/v1/P18-1079",
    pages = "856--865",
    abstract = "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) {--} semantic-preserving perturbations that induce changes in the model{'}s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) {--} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",
}

@article{belinkov2018breaknmt,
  author    = {Yonatan Belinkov and
               Yonatan Bisk},
  title     = {Synthetic and Natural Noise Both Break Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1711.02173},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.02173},
  eprinttype = {arXiv},
  eprint    = {1711.02173},
  timestamp = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-02173.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{rychalska2019wildnlp,
author="Rychalska, Barbara
and Basaj, Dominika
and Gosiewska, Alicja
and Biecek, Przemys{\l}aw",
editor="Gedeon, Tom
and Wong, Kok Wai
and Lee, Minho",
title="Models in the Wild: On Corruption Robustness of Neural NLP Systems",
booktitle="Neural Information Processing",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="235--247",
abstract="Natural Language Processing models lack a unified approach to robustness testing. In this paper we introduce WildNLP - a framework for testing model stability in a natural setting where text corruptions such as keyboard errors or misspelling occur. We compare robustness of deep learning models from 4 popular NLP tasks: Q{\&}A, NLI, NER and Sentiment Analysis by testing their performance on aspects introduced in the framework. In particular, we focus on a comparison between recent state-of-the-art text representations and non-contextualized word embeddings. In order to improve robustness, we perform adversarial training on selected aspects and check its transferability to the improvement of models with various corruption types. We find that the high performance of models does not ensure sufficient robustness, although modern embedding techniques help to improve it. We release the code of WildNLP framework for the community.",
isbn="978-3-030-36718-3"
}

@inproceedings{iyyer2018adversarial,
    title = "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks",
    author = "Iyyer, Mohit  and
      Wieting, John  and
      Gimpel, Kevin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1170",
    doi = "10.18653/v1/N18-1170",
    pages = "1875--1885",
    abstract = "We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) {``}fool{''} pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.",
}

@article{prabhakaran2019fairness,
  author    = {Vinodkumar Prabhakaran and
               Ben Hutchinson and
               Margaret Mitchell},
  title     = {Perturbation Sensitivity Analysis to Detect Unintended Model Biases},
  journal   = {CoRR},
  volume    = {abs/1910.04210},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.04210},
  eprinttype = {arXiv},
  eprint    = {1910.04210},
  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-04210.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{ribeiro2019consistencyeval,
    title = "Are Red Roses Red? Evaluating Consistency of Question-Answering Models",
    author = "Ribeiro, Marco Tulio  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1621",
    doi = "10.18653/v1/P19-1621",
    pages = "6174--6184",
    abstract = "Although current evaluation of question-answering systems treats predictions in isolation, we need to consider the relationship between predictions to measure true understanding. A model should be penalized for answering {``}no{''} to {``}Is the rose red?{''} if it answers {``}red{''} to {``}What color is the rose?{''}. We propose a method to automatically extract such implications for instances from two QA datasets, VQA and SQuAD, which we then use to evaluate the consistency of models. Human evaluation shows these generated implications are well formed and valid. Consistency evaluation provides crucial insights into gaps in existing models, while retraining with implication-augmented data improves consistency on both synthetic and human-generated implications.",
}

@article{ribeiroSG16lime,
  author    = {Marco T{\'{u}}lio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  title     = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  journal   = {CoRR},
  volume    = {abs/1602.04938},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.04938},
  eprinttype = {arXiv},
  eprint    = {1602.04938},
  timestamp = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RibeiroSG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2018glue,
  author    = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1804.07461},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.07461},
  eprinttype = {arXiv},
  eprint    = {1804.07461},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{devlin2019bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2019roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}

@article{mitchell1993treebank,
author = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
title = {Building a Large Annotated Corpus of English: The Penn Treebank},
year = {1993},
issue_date = {June 1993},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {19},
number = {2},
issn = {0891-2017},
journal = {Comput. Linguist.},
month = {jun},
pages = {313–330},
numpages = {18}
}

@online{nltkTreebankCorporaWebPage,
  author = {Sphinx and NLTK Theme},
  title = {NLTK Documentation},
  key = {NLTKDocumentationWebPage},
  year = {2021},
  note = {\url{https://www.nltk.org/howto/corpus.html}},
}

@inproceedings{kitaev2019multilingual,
    title = "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
    author = "Kitaev, Nikita  and
      Cao, Steven  and
      Klein, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1340",
    doi = "10.18653/v1/P19-1340",
    pages = "3499--3505",
}

@inproceedings{kitaev2018constituency,
    title = "Constituency Parsing with a Self-Attentive Encoder",
    author = "Kitaev, Nikita  and
      Klein, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1249",
    doi = "10.18653/v1/P18-1249",
    pages = "2676--2686",
}

@inproceedings{baccianella2010sentiwordnet,
    title = "{S}enti{W}ord{N}et 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining",
    author = "Baccianella, Stefano  and
      Esuli, Andrea  and
      Sebastiani, Fabrizio",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/769_Paper.pdf",
    abstract = "In this work we present SENTIWORDNET 3.0, a lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide. Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET 2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores. We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0. We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20{\%} with respect to SENTIWORDNET 1.0.",
}

@Article{mihaela2017sentiwordnetlabel,
AUTHOR = {Colhon, Mihaela and Vlăduţescu, Ştefan and Negrea, Xenia},
TITLE = {How Objective a Neutral Word Is? A Neutrosophic Approach for the Objectivity Degrees of Neutral Words},
JOURNAL = {Symmetry},
VOLUME = {9},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {280},
URL = {https://www.mdpi.com/2073-8994/9/11/280},
ISSN = {2073-8994},
ABSTRACT = {In the latest studies concerning the sentiment polarity of words, the authors mostly consider the positive and negative constructions, without paying too much attention to the neutral words, which can have, in fact, significant sentiment degrees. More precisely, not all the neutral words have zero positivity or negativity scores, some of them having quite important nonzero scores for these polarities. At this moment, in the literature, a word is considered neutral if its positive and negative scores are equal, which implies two possibilities: (1) zero positive and negative scores; (2) nonzero, but equal positive and negative scores. It is obvious that these cases represent two different categories of neutral words that must be treated separately by a sentiment analysis task. In this paper, we present a comprehensive study about the neutral words applied to English as is developed with the aid of SentiWordNet 3.0: the publicly available lexical resource for opinion mining. We designed our study in order to provide an accurate classification of the so-called “neutral words” described in terms of sentiment scores and using measures from neutrosophy theory. The intended scope is to fill the gap concerning the neutrality aspect by giving precise measurements for the words’ objectivity.},
DOI = {10.3390/sym9110280}
}

@inproceedings{kitaev2019seedparser,
    title = "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
    author = "Kitaev, Nikita  and
      Cao, Steven  and
      Klein, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1340",
    doi = "10.18653/v1/P19-1340",
    pages = "3499--3505",
}

@inproceedings{kitaev2018seedparser,
    title = "Constituency Parsing with a Self-Attentive Encoder",
    author = "Kitaev, Nikita  and
      Klein, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1249",
    doi = "10.18653/v1/P18-1249",
    pages = "2676--2686",
}

@inproceedings{pei2017deepxplore,
	doi = {10.1145/3132747.3132785},
	url = {https://doi.org/10.1145%2F3132747.3132785},
	year = 2017,
	month = {oct},
	publisher = {{ACM}},
	author = {Kexin Pei and Yinzhi Cao and Junfeng Yang and Suman Jana},
	title = {{DeepXplore}}, 
	booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles}
}

@inbook{simin2020denas,
author = {Chen, Simin and Bateni, Soroush and Grandhi, Sampath and Li, Xiaodi and Liu, Cong and Yang, Wei},
title = {DENAS: Automated Rule Generation by Knowledge Extraction from Neural Networks},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409733},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {813–825},
numpages = {13}
}

@inproceedings{patel2018mlevalforsoftware,
author = {Patel, Kayur and Fogarty, James and Landay, James A. and Harrison, Beverly},
title = {Investigating Statistical Machine Learning as a Tool for Software Development},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1357054.1357160},
doi = {10.1145/1357054.1357160},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {667–676},
numpages = {10},
keywords = {statistical machine learning, software development},
location = {Florence, Italy},
series = {CHI '08}
}



@inproceedings{ma2018deepgauge,
  title={Deepgauge: Multi-granularity testing criteria for deep learning systems},
  author={Ma, Lei and Juefei-Xu, Felix and Zhang, Fuyuan and Sun, Jiyuan and Xue, Minhui and Li, Bo and Chen, Chunyang and Su, Ting and Li, Li and Liu, Yang and others},
  booktitle={Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  pages={120--131},
  year={2018}
}

@ARTICLE{segura2016metamorphictest,
  author={Segura, Sergio and Fraser, Gordon and Sanchez, Ana B. and Ruiz-Cortés, Antonio},
  journal={IEEE Transactions on Software Engineering}, 
  title={A Survey on Metamorphic Testing}, 
  year={2016},
  volume={42},
  number={9},
  pages={805-824},
  doi={10.1109/TSE.2016.2532875}
}

@inproceedings{lime,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}
@inproceedings{lemna,
  title={Lemna: Explaining deep learning based security applications},
  author={Guo, Wenbo and Mu, Dongliang and Xu, Jun and Su, Purui and Wang, Gang and Xing, Xinyu},
  booktitle={proceedings of the 2018 ACM SIGSAC conference on computer and communications security},
  pages={364--379},
  year={2018}
}

@article{husnain2021swnvalidity,
  title={A systematic study on the role of SentiWordNet in opinion mining},
  author={Husnain, Mujtaba and Missen, Malik Muhammad Saad and Akhtar, Nadeem and Coustaty, Micka{\"e}l and Mumtaz, Shahzad and Prasath, VB},
  journal={Frontiers of Computer Science},
  volume={15},
  number={4},
  pages={1--19},
  year={2021},
  publisher={Springer}
}

@article{wang2019superglue,
  author    = {Alex Wang and
               Yada Pruksachatkun and
               Nikita Nangia and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {SuperGLUE: {A} Stickier Benchmark for General-Purpose Language Understanding
               Systems},
  journal   = {CoRR},
  volume    = {abs/1905.00537},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.00537},
  eprinttype = {arXiv},
  eprint    = {1905.00537},
  timestamp = {Mon, 27 May 2019 13:15:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00537.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pinjia2020structinvtestingnmt,
  author    = {Pinjia He and
               Clara Meister and
               Zhendong Su},
  title     = {Structure-Invariant Testing for Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1907.08710},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.08710},
  eprinttype = {arXiv},
  eprint    = {1907.08710},
  timestamp = {Tue, 30 Nov 2021 15:21:24 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-08710.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pinjia2020testnmtrt,
  author    = {Pinjia He and
               Clara Meister and
               Zhendong Su},
  title     = {Testing Machine Translation via Referential Transparency},
  journal   = {CoRR},
  volume    = {abs/2004.10361},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.10361},
  eprinttype = {arXiv},
  eprint    = {2004.10361},
  timestamp = {Tue, 30 Nov 2021 15:21:24 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-10361.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
 }

@inproceedings{socher2013sst,
    title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
    author = {Socher, Richard  and Perelygin, Alex  and      Wu, Jean  and    Chuang, Jason  and  Manning, Christopher D.  and    Ng, Andrew  and   Potts, Christopher},
    booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
    year = {2013},
    address = {Seattle, Washington, USA},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/D13-1170},
    pages = {1631--1642},
}

@unpublished{spacy,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}

@article{zylberajch2021hildif,
  title={HILDIF: Interactive Debugging of NLI Models Using Influence Functions},
  author={Hugo Zylberajch and Piyawat Lertvittayakumjorn and Francesca Toni},
  journal={Proceedings of the First Workshop on Interactive Learning
                  for Natural Language Processing},
  year={2021}
}

@misc{lertvittayakumjorn2020find,
  doi = {10.48550/ARXIV.2010.04987},
  
  url = {https://arxiv.org/abs/2010.04987},
  
  author = {Lertvittayakumjorn, Piyawat and Specia, Lucia and Toni, Francesca},
  
  keywords = {Computation and Language (cs.CL), Human-Computer Interaction (cs.HC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {FIND: Human-in-the-Loop Debugging Deep Text Classifiers},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{qizhe2019unsupdataaug,
  author    = {Qizhe Xie and
               Zihang Dai and
               Eduard H. Hovy and
               Minh{-}Thang Luong and
               Quoc V. Le},
  title     = {Unsupervised Data Augmentation},
  journal   = {CoRR},
  volume    = {abs/1904.12848},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.12848},
  eprinttype = {arXiv},
  eprint    = {1904.12848},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-12848.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{coulombe2018textaugapi,
  author    = {Claude Coulombe},
  title     = {Text Data Augmentation Made Simple By Leveraging {NLP} Cloud APIs},
  journal   = {CoRR},
  volume    = {abs/1812.04718},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.04718},
  eprinttype = {arXiv},
  eprint    = {1812.04718},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-04718.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{weizou2019eda,
    title = "{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
    author = "Wei, Jason  and
      Zou, Kai",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1670",
    doi = "10.18653/v1/D19-1670",
    pages = "6382--6388",
    abstract = "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50{\%} of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.",
}

@article{hongyu2019dataaugmixup,
  author    = {Hongyu Guo and
               Yongyi Mao and
               Richong Zhang},
  title     = {Augmenting Data with Mixup for Sentence Classification: An Empirical
               Study},
  journal   = {CoRR},
  volume    = {abs/1905.08941},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.08941},
  eprinttype = {arXiv},
  eprint    = {1905.08941},
  timestamp = {Wed, 29 May 2019 11:27:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-08941.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ateret2019lambda,
  author    = {Ateret Anaby{-}Tavor and
               Boaz Carmeli and
               Esther Goldbraich and
               Amir Kantor and
               George Kour and
               Segev Shlomov and
               Naama Tepper and
               Naama Zwerdling},
  title     = {Not Enough Data? Deep Learning to the Rescue!},
  journal   = {CoRR},
  volume    = {abs/1911.03118},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.03118},
  eprinttype = {arXiv},
  eprint    = {1911.03118},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-03118.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhu2018texygen,
  title={Texygen: A Benchmarking Platform for Text Generation Models},
  author={Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  journal={SIGIR},
  year={2018}
}

@article{morris2020textattack,
  author    = {John X. Morris and
               Eli Lifland and
               Jin Yong Yoo and
               Yanjun Qi},
  title     = {TextAttack: {A} Framework for Adversarial Attacks in Natural Language
               Processing},
  journal   = {CoRR},
  volume    = {abs/2005.05909},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.05909},
  eprinttype = {arXiv},
  eprint    = {2005.05909},
  timestamp = {Thu, 14 Oct 2021 09:15:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-05909.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zang2020sememepso,
    title = "Word-level Textual Adversarial Attacking as Combinatorial Optimization",
    author = "Zang, Yuan  and
      Qi, Fanchao  and
      Yang, Chenghao  and
      Liu, Zhiyuan  and
      Zhang, Meng  and
      Liu, Qun  and
      Sun, Maosong",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.540",
    doi = "10.18653/v1/2020.acl-main.540",
    pages = "6066--6080",
    abstract = "Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.",
}

@inproceedings{alzantot2018genadvexp,
    title = "Generating Natural Language Adversarial Examples",
    author = "Alzantot, Moustafa  and
      Sharma, Yash  and
      Elgohary, Ahmed  and
      Ho, Bo-Jhang  and
      Srivastava, Mani  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1316",
    doi = "10.18653/v1/D18-1316",
    pages = "2890--2896",
    abstract = "Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97{\%} and 70{\%}, respectively. We additionally demonstrate that 92.3{\%} of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.",
}

@article{linyang2020bertattack,
  author    = {Linyang Li and
               Ruotian Ma and
               Qipeng Guo and
               Xiangyang Xue and
               Xipeng Qiu},
  title     = {{BERT-ATTACK:} Adversarial Attack Against {BERT} Using {BERT}},
  journal   = {CoRR},
  volume    = {abs/2004.09984},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.09984},
  eprinttype = {arXiv},
  eprint    = {2004.09984},
  timestamp = {Tue, 28 Apr 2020 16:10:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-09984.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{udeshi2019ogma,
  author    = {Sakshi Udeshi and
               Sudipta Chattopadhyay},
  title     = {Grammar Based Directed Testing of Machine Learning Systems},
  journal   = {CoRR},
  volume    = {abs/1902.10027},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.10027},
  eprinttype = {arXiv},
  eprint    = {1902.10027},
  timestamp = {Tue, 21 May 2019 18:03:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-10027.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{soremekun2020astraea,
  author    = {Ezekiel O. Soremekun and
               Sakshi Udeshi and
               Sudipta Chattopadhyay},
  title     = {Astraea: Grammar-based Fairness Testing},
  journal   = {CoRR},
  volume    = {abs/2010.02542},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.02542},
  eprinttype = {arXiv},
  eprint    = {2010.02542},
  timestamp = {Thu, 14 Oct 2021 09:13:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-02542.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{udeshi2018fairnesstest,
  author    = {Sakshi Udeshi and
               Pryanshu Arora and
               Sudipta Chattopadhyay},
  title     = {Automated Directed Fairness Testing},
  journal   = {CoRR},
  volume    = {abs/1807.00468},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.00468},
  eprinttype = {arXiv},
  eprint    = {1807.00468},
  timestamp = {Wed, 10 Oct 2018 08:05:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-00468.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}