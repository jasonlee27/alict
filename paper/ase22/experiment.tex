\section{Experiment}
\label{sec:experiment}
%

\InputWithSpace{tables/retrain-debug-table}

In this section, we present experiments to evaluate the effectiveness
of our proposed evaluation methodology. In particular, we address the
following research questions (RQs):

\begin{enumerate}[label=\textbf{RQ\arabic*}]
\item \label{rq:one}: Can \tool generate sentences consistent to their
  sentiment label and their target \lc?
\item \label{rq:two}: Can the \tool generated testcases have stronger
  generalization ability to test \sa models?
\item \label{rq:three}: Can \tool be useful to find root of bug in the
  \sa models?
%% \item \label{rq:three}: How effective is our new test case generation
%%   using \cfg expansion? % ablation study
\end{enumerate}

To answer the RQs, we need to show: (\romnum{1}) correctness of testcase to
evaluate its target \lc.  (\romnum{2}) effect of testcase distribution on
finding bugs; (\romnum{3}) degree of execution of a \sa model on testcases;
and (\romnum{4}) ability to guide to find cause of bug in a \sa
model.

First, \tool generates sentences and its label by empirical reasoning
derived from \lc statements. Therefore, we meaure acceptibility of the
testcases by verifying the relevancy between sentence and label, and
between sentence and its \lc. The RQ~\ref{rq:one} thereby is answered
by verifying the quality of generated testcases. In this work, we
conduct manual study and measure the correlation of human labeled
testcases and \tool labeled testcases. Second, the amount of \sa model
component executed during testing is a critical measurement for
assessing quality of software testing. A high software coverage
results in higher chances of unidentified bugs in the \sa model. On
the other hand, limited distribution only represents narrow portion of
real world covering limited execution behaviors in a \sa model. It leads
to detect bugs within the restricted execution behaviors.  Therefore,
testcases more equivalent to real world represents more generalized
distribution, and it results in high coverage of the \sa model.
Accordingly, the ability of testcases to produce high portion of model
coverage needs to be measured. We answer the RQ~\ref{rq:three}
measuring neural coverage of the neural \sa model according to the
measurements. In addition to the detection of bugs in the model,
explaination of the bugs is also important stage of debugging and
repairing the model. Therefore, we anwer RQ~\ref{rq:three} by showing
usefulness of \tool by analyzing the \sa model based on \tool and
finding root of the bug.

\subsection{Experiment Setup}
We conduct practical experiments and show the effectiveness of
\tool.
\subsubsection*{\textbf{Manual Study}}
We sample set of test cases and corresponding \lc randomly sampled
from seed sentences and expanded sentences from \tool.  Subjects are
also provided with the sampled testcases. Each subject is asked for
scoring relevancy between the linguistic capability of the
sentences. The score is discrete and it ranges from 1 to 5
representing ``strongly not relevant'' to ``strongly relevant''
respectively. Additionally, the subject scores the relevancy between
sentences and their sentiment labels. The score is also discreate and
ranges from 1 to 5. It represents ``strongly negative'' to ``strongly
positive'' respectively. In this work, we collect manual study scores
from 8 subjects attended. From the collected scores, we measure the
following metrics:

\begin{eqnarray}
  &Reported\:bugs &=\sum_{i} \delta(label_{S^2LCT}!=pred_{model}) \label{metric:rb}\\
  &Incorrect\:inputs &= \sum_{i} \delta(pred_{model}!=label_{human}) \label{metric:ii}\\
  %% &Label\:inconsistency &= \sum_{i} \delta(label_{S^2LCT}!=label_{human}) \label{metric:li}
\end{eqnarray}

The metrics represents the number of data instance that its label
assigned from are different between \tool and its model prediction
(eq~\ref{metric:rb}) and between the model prediction and human
(eq~\ref{metric:ii}). We show the correspondonce between \tool and
human by computing the precision and recall from the reported bugs and
incorrect inputs.
%% and between \tool and human (eq~\ref{metric:li}).

\subsubsection*{\textbf{Model Coverage}}
We implement DeepXplore to measure the \sa model
coverage~\cite{pei2017deepxplore}. \Dxp is the first efficient
whitebox testing framework for large-scale \dl systems. It introduces
neuron coverage of a set of test inputs as the ratio of the number of
unique activated neurons and the total number of neurons in input \dl
system. In this experiment, we compute the neuron coverage of a test
cases from \tool and \Cklst on the fine-tuned \sa model of
\bertsamodel and compare the coverage between \tool and \Cklst.

\subsubsection*{\textbf{Analysis of Root of bug}}
In this experiment, we analyze a \sa model based on the test results
of the set of \tool test inputs and find the root of bug in the
model. It shows that \tool generated testcases are useful to explain
the \sa model test results and identify the rationale of the bug
found. We implement \Denas for this experiment. Rules generated from
\Denas is interpretable functions mapping certain features of the
input to the expected output of a deep \nn system, and the generated
rules are considered as the behaviors of the deep \nn system. To
anlayze the bug of a \sa model, we generates rules over the test
inputs using \Denas and identifies falty rules for the failed test
inputs. We manually identify the root of the falty rules and find the
root of bug in the end.

%% \MyPara{Seed Input Selection}
%% %
%% For each linguistic capability, we first search all sentences that
%% meet its requirement. Among found sentences, we randomly select 10
%% sentences due to memory constraint.

%% \MyPara{Word Sentiment}
%% %
%% we extract sentiments of words using the
%% \Swn~\cite{baccianella2010sentiwordnet}. The \Swn is a publicly
%% available lexical resource of words on Wordnet with three numerical
%% scores of objectivity, positivity and negativity. Sentiment word
%% labels from the scores are classified from the algorithm from Mihaela
%% \etal~\cite{mihaela2017sentiwordnetlabel}.

%% \MyPara{\Cfg Expansion}
%% %
%% We build a reference \Cfg of natural language from the English Penn
%% \Trb corpora~\cite{mitchell1993treebank,nltkTreebankCorporaWebPage}.
%% The corpus is sampled from 2,499 stories from a tree year \Wsj
%% collection The \Trb provides a parsed text corpus with annotation of
%% syntactic and semantic structure. In this experiment We implement the
%% \trb corpora available through \Nltk, which is a suite of libraries
%% and programs for \Nlp for English. In addition, we parse the seed
%% input using into its CFG using the Berkeley Neural
%% Parser~\cite{kitaev2018constituency, kitaev2019multilingual}, a
%% high-accuracy parser with models for 11 languages. The input is a raw
%% text in natural language and the output is the string representation of
%% parse tree. Next after comparing CFGs between reference and seed input,
%% we randomly select 10 expansions for generating templates due to
%% memory constraint.

%% \MyPara{Synonyms}
%% %
%% \Model searches synonyms of each token from synonym sets extracted
%% from \Wrdnt using \Spacy open-source library for NLP.

%% \MyPara{Models}
%% %
%% We evaluate the following \sa models via \Model:
%% \Bert~\cite{devlin2019bert}, \Roberta~\cite{liu2019roberta} and
%% \Dbert~\cite{sanh2019distilbert}. These models are fine-tuned on \Sstt
%% and their accuracies are \BertAcc, \RobertaAcc and \DbertAcc.

%% \MyPara{Retraining}
%% %
%% We retrain \sa models. we split \Model generated test cases into
%% train/validation/test sets with the ratio of 8:1:1. The number of
%% epochs and batch size for retraining are 1 and 16 respectively.
