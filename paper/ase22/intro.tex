\section{Introduction}
\label{sec:intro}

Software testing is the essential component of software development
process.  It evaluates an attribute or capability of the software and
determines correctness of the software by examining the behavior of
the software and comparing it with the expected outcome. Software
testing in the early stage of the development identifies errors,
faults, defects, and fixing them saves amount of costs. Therefore,
reliable software testing methodology assures quality in software, and
it meets needs of the end user. Meanwhile, the trustworthiness in
quality of \nlp (NLP) application also becomes important component for
its practical use in real world. Traditionally, the prevalent models
of NLP are evaluated via train-validation-test splits. train and
validation set is used to train the NLP model and the \ho set is used
for testing by measuring accuracy. In this method, the accuracy is a
indicator of the performance of the models, and model with higher
accuracy becomes better model.

Despite its simplilcity and usefulness, the testing paradigm often
overestimates the performances from the \ho set. Generalization does
not hold in the \ho dataset and it is likely to introduce specific
biases. The biases increase the discrepancy of distribution between
the dataset and real world~\cite{recht2019imagenetbias}. Therefore,
the \ho set does not represent the real world and the accuracy from
the dataset fails to measure comprehensive performrance of the NLP
model. In addition, the aggregated accuracy metric does not validate
certain behaviors of the model, and, as a consequence, it causes
costly to locate where the errors comes
from~\cite{wu2019errudite}. Therefore, on the subject of the
limitation of traditional testing paradigm, a number of methods have
been proposed for evaluating multiple perspectives on the NLP model
such as robustness on adversarial examples, model coverage and
fairness.  Especially, Ribeiro~\etal intoduced the \Cklst, a
behavioral testing framework for evaluating NLP model on multiple
\lcs~\cite{marcoACL2020checklist}. The \Cklst provides predefined
task-relevant \lcs and generate testcases relevant to each \lc.
However, the approach only relies on manually generated input
templates, thus the template generation becomes expensive and time
consuming. In addition, the generated templates are selective and
often too simple, and it is limited to provide restricted evaluation
of linguistic capabilities. Thus, it does not garauntee the
comprehensive evaluation.

Notwithstanding, the evaluating the NLP model on the \lcs enables
comprehensive testing of the model. Each \lc explain the functionality
of input and outpus behavior for the NLP model under test. the
expected bahavior is determined by combining the input and output
scopes specified on the \lc. Typically, it describes certain type of
input and outputs observed in real world for the target NLP task
ranging from simple to complicated behaviors so that they are able to
evaluate the NLP model comprehensively. Such evaluation on the
specified functionalities avoids the overestimation of the model
performance as it equivalently measures the model performance on each
functionality, and the seperate model performance explain distribution
of the performaces over the \lcs. In the end, it provides not only the
overall model performance, but also the malfunction facets of the
model. However, the prior work currently generated model.

In this paper, we present \tool, an automated NLP model evaluation
method for comprehensive behavioral testing of NLP models on \sa
task. For each behavior of linguistic capability, \tool does not rely
on the manual input generation. Instead, it establishes input
requirement for evaluating a linguistic capability and finds suitable
inputs that meet the requirement from existing public dataset.
Therefore, \tool increases input diversity and generality. Further,
\tool applies the fuzzing testing principle to generate inputs by
mutating the selected inputs as seed inputs. Fuzzer in \tool first
expands seed input grammar structures and determines its available
\pos to maintain structural naturalness. After that, to hold
contextual naturalness of the mutated inputs, the fuzzer completes the
expanded new structures via data-driven context-aware word
suggestion. Additionally, sentiment-independent words in the inputs
are replaced with rule-based word suggestion.

We demonstrate its generality and utility as a NLP model evaluation
tool by evaluating well-known \sa models: \Bert~\cite{devlin2019bert},
\Roberta~\cite{liu2019roberta} and \Dbert~\cite{sanh2019distilbert}.
We show that
