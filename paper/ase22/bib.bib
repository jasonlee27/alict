@String{AAAI = "AAAI Conference on Artificial Intelligence"}
@String{ACL = "Annual Meeting of the Association for Computational Linguistics"}
@String{AOSD = "International Conference on Aspect-Oriented Software Development"}
@String{ASE = "Automated Software Engineering"}
@String{ASETool = "Automated Software Engineering, Tool Demonstrations"}
@String{CADE = "International Conference on Automated Deduction"}
@String{CAV = "International Conference on Computer Aided Verification"}
@String{CICLing = "International Conference on Intelligent Text Processing and Computational Linguistics"}
@String{CICM = "International Conference on Intelligent Computer Mathematics"}
@String{CICMWIP = "Work in Progress at the Conference on Intelligent Computer Mathematics"}
@String{COLING = "International Conference on Computational Linguistics"}
@String{COQPL = "International Workshop on Coq for Programming Languages"}
@String{CPP = "Certified Programs and Proofs"}
@String{CSUR = "ACM Computing Surveys"}
@String{EACL = "Conference of the European Chapter of the Association for Computational Linguistics"}
@String{ECOOP = "European Conference on Object-Oriented Programming"}
@String{EMNLP = "Empirical Methods in Natural Language Processing"}
@String{ESEC/FSE = "Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering"}
@String{ESOP = "European Symposium on Programming"}
@String{FASE = "Fundamental Approaches to Software Engineering"}
@String{FM = "International Symposium on Formal Methods"}
@String{FSE = "International Symposium on the Foundations of Software Engineering"}
@String{FSENIER = "International Symposium on the Foundations of Software Engineering, NIER"}
@String{ICLR = "International Conference on Learning Representations"}
@String{ICML = "International Conference on Machine Learning"}
@String{ICPC = "International Conference on Program Comprehension"}
@String{ICSE = "International Conference on Software Engineering"}
@String{ICSME = "International Conference on Software Maintenance and Evolution"}
@String{ICST = "International Conference on Software Testing, Verification, and Validation"}
@String{IJCAI = "International Joint Conference on Artificial Intelligence"}
@String{IJCAR = "International Joint Conference on Automated Reasoning"}
@String{ISSTA = "International Symposium on Software Testing and Analysis"}
@String{ITP = "International Conference on Interactive Theorem Proving"}
@String{LPAR = "International Conference on Logic for Programming, Artificial Intelligence, and Reasoning"}
@String{MSR = "International Conference on Mining Software Repositories"}
@String{NAACL = "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}
@String{NeurIPS = "Annual Conference on Neural Information Processing Systems"}
@String{NIPS = "Annual Conference on Neural Information Processing Systems"}
@String{OOPSLA = "International Conference on Object-Oriented Programming, Systems, Languages, and Applications"}
@String{PLDI = "Conference on Programming Language Design and Implementation"}
@String{POPL = "Symposium on Principles of Programming Languages"}
@String{SANER = "International Conference on Software Analysis, Evolution and Reengineering"}
@String{SOOPPA = "Symposium on Object-Oriented Programming Emphasizing Practical Applications"}
@String{SOSP = "Symposium on Operating Systems Principles"}
@String{TPHOLs = "Theorem Proving in Higher Order Logics"}
@String{TYPES = "International Conference on Types for Proofs and Programs"}
@String{UITP = "International Workshop On User Interfaces for Theorem Provers"}
@String{WCRE = "Working Conference on Reverse Engineering"}


@InProceedings{recht2019imagenetbias,
  title = 	 {Do {I}mage{N}et Classifiers Generalize to {I}mage{N}et?},
  author =       {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5389--5400},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/recht19a/recht19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/recht19a.html},
  abstract = 	 {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly "harder" images than those found in the original test sets.}
}

@inproceedings{wu2019errudite,
    title = "{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",
    author = "Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1073",
    doi = "10.18653/v1/P19-1073",
    pages = "747--763",
    abstract = "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",
}

@inproceedings{marcoACL2020checklist,  
 author = {Marco Tulio Ribeiro and Tongshuang Wu and Carlos Guestrin and Sameer Singh},  
 title = {Beyond Accuracy: Behavioral Testing of NLP models with CheckList},  
 booktitle = {Association for Computational Linguistics (ACL)},  
 year = {2020}
}

@inproceedings{ribeiro2018sear,
    title = "Semantically Equivalent Adversarial Rules for Debugging {NLP} models",
    author = "Ribeiro, Marco Tulio  and
      Singh, Sameer  and
      Guestrin, Carlos",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1079",
    doi = "10.18653/v1/P18-1079",
    pages = "856--865",
    abstract = "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) {--} semantic-preserving perturbations that induce changes in the model{'}s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) {--} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",
}

@article{belinkov2018breaknmt,
  author    = {Yonatan Belinkov and
               Yonatan Bisk},
  title     = {Synthetic and Natural Noise Both Break Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1711.02173},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.02173},
  eprinttype = {arXiv},
  eprint    = {1711.02173},
  timestamp = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-02173.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{rychalska2019wildnlp,
author="Rychalska, Barbara
and Basaj, Dominika
and Gosiewska, Alicja
and Biecek, Przemys{\l}aw",
editor="Gedeon, Tom
and Wong, Kok Wai
and Lee, Minho",
title="Models in the Wild: On Corruption Robustness of Neural NLP Systems",
booktitle="Neural Information Processing",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="235--247",
abstract="Natural Language Processing models lack a unified approach to robustness testing. In this paper we introduce WildNLP - a framework for testing model stability in a natural setting where text corruptions such as keyboard errors or misspelling occur. We compare robustness of deep learning models from 4 popular NLP tasks: Q{\&}A, NLI, NER and Sentiment Analysis by testing their performance on aspects introduced in the framework. In particular, we focus on a comparison between recent state-of-the-art text representations and non-contextualized word embeddings. In order to improve robustness, we perform adversarial training on selected aspects and check its transferability to the improvement of models with various corruption types. We find that the high performance of models does not ensure sufficient robustness, although modern embedding techniques help to improve it. We release the code of WildNLP framework for the community.",
isbn="978-3-030-36718-3"
}

@inproceedings{iyyer2018adversarial,
    title = "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks",
    author = "Iyyer, Mohit  and
      Wieting, John  and
      Gimpel, Kevin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1170",
    doi = "10.18653/v1/N18-1170",
    pages = "1875--1885",
    abstract = "We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) {``}fool{''} pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.",
}

@article{prabhakaran2019fairness,
  author    = {Vinodkumar Prabhakaran and
               Ben Hutchinson and
               Margaret Mitchell},
  title     = {Perturbation Sensitivity Analysis to Detect Unintended Model Biases},
  journal   = {CoRR},
  volume    = {abs/1910.04210},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.04210},
  eprinttype = {arXiv},
  eprint    = {1910.04210},
  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-04210.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{rottger2020hatecheck,
  author    = {Paul R{\"{o}}ttger and
               Bertram Vidgen and
               Dong Nguyen and
               Zeerak Waseem and
               Helen Z. Margetts and
               Janet B. Pierrehumbert},
  title     = {HateCheck: Functional Tests for Hate Speech Detection Models},
  journal   = {CoRR},
  volume    = {abs/2012.15606},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.15606},
  eprinttype = {arXiv},
  eprint    = {2012.15606},
  timestamp = {Fri, 15 Jan 2021 16:55:50 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-15606.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ribeiro2019consistencyeval,
    title = "Are Red Roses Red? Evaluating Consistency of Question-Answering Models",
    author = "Ribeiro, Marco Tulio  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1621",
    doi = "10.18653/v1/P19-1621",
    pages = "6174--6184",
    abstract = "Although current evaluation of question-answering systems treats predictions in isolation, we need to consider the relationship between predictions to measure true understanding. A model should be penalized for answering {``}no{''} to {``}Is the rose red?{''} if it answers {``}red{''} to {``}What color is the rose?{''}. We propose a method to automatically extract such implications for instances from two QA datasets, VQA and SQuAD, which we then use to evaluate the consistency of models. Human evaluation shows these generated implications are well formed and valid. Consistency evaluation provides crucial insights into gaps in existing models, while retraining with implication-augmented data improves consistency on both synthetic and human-generated implications.",
}

@article{ribeiroSG16lime,
  author    = {Marco T{\'{u}}lio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  title     = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  journal   = {CoRR},
  volume    = {abs/1602.04938},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.04938},
  eprinttype = {arXiv},
  eprint    = {1602.04938},
  timestamp = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RibeiroSG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2018glue,
  author    = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1804.07461},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.07461},
  eprinttype = {arXiv},
  eprint    = {1804.07461},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wu2019errudite,
    title = "{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",
    author = "Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1073",
    doi = "10.18653/v1/P19-1073",
    pages = "747--763",
    abstract = "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",
}

@article{devlin2019bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2019roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}

@article{mitchell1993treebank,
author = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
title = {Building a Large Annotated Corpus of English: The Penn Treebank},
year = {1993},
issue_date = {June 1993},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {19},
number = {2},
issn = {0891-2017},
journal = {Comput. Linguist.},
month = {jun},
pages = {313–330},
numpages = {18}
}

@online{nltkTreebankCorporaWebPage,
  author = {Sphinx and NLTK Theme},
  title = {NLTK Documentation},
  key = {NLTKDocumentationWebPage},
  year = {2021},
  note = {\url{https://www.nltk.org/howto/corpus.html}},
}

@inproceedings{kitaev2019multilingual,
    title = "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
    author = "Kitaev, Nikita  and
      Cao, Steven  and
      Klein, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1340",
    doi = "10.18653/v1/P19-1340",
    pages = "3499--3505",
}

@inproceedings{kitaev2018constituency,
    title = "Constituency Parsing with a Self-Attentive Encoder",
    author = "Kitaev, Nikita  and
      Klein, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1249",
    doi = "10.18653/v1/P18-1249",
    pages = "2676--2686",
}

@inproceedings{baccianella2010sentiwordnet,
    title = "{S}enti{W}ord{N}et 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining",
    author = "Baccianella, Stefano  and
      Esuli, Andrea  and
      Sebastiani, Fabrizio",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/769_Paper.pdf",
    abstract = "In this work we present SENTIWORDNET 3.0, a lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide. Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET 2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores. We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0. We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20{\%} with respect to SENTIWORDNET 1.0.",
}

@Article{mihaela2017sentiwordnetlabel,
AUTHOR = {Colhon, Mihaela and Vlăduţescu, Ştefan and Negrea, Xenia},
TITLE = {How Objective a Neutral Word Is? A Neutrosophic Approach for the Objectivity Degrees of Neutral Words},
JOURNAL = {Symmetry},
VOLUME = {9},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {280},
URL = {https://www.mdpi.com/2073-8994/9/11/280},
ISSN = {2073-8994},
ABSTRACT = {In the latest studies concerning the sentiment polarity of words, the authors mostly consider the positive and negative constructions, without paying too much attention to the neutral words, which can have, in fact, significant sentiment degrees. More precisely, not all the neutral words have zero positivity or negativity scores, some of them having quite important nonzero scores for these polarities. At this moment, in the literature, a word is considered neutral if its positive and negative scores are equal, which implies two possibilities: (1) zero positive and negative scores; (2) nonzero, but equal positive and negative scores. It is obvious that these cases represent two different categories of neutral words that must be treated separately by a sentiment analysis task. In this paper, we present a comprehensive study about the neutral words applied to English as is developed with the aid of SentiWordNet 3.0: the publicly available lexical resource for opinion mining. We designed our study in order to provide an accurate classification of the so-called “neutral words” described in terms of sentiment scores and using measures from neutrosophy theory. The intended scope is to fill the gap concerning the neutrality aspect by giving precise measurements for the words’ objectivity.},
DOI = {10.3390/sym9110280}
}

@inproceedings{kitaev2019seedparser,
    title = "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
    author = "Kitaev, Nikita  and
      Cao, Steven  and
      Klein, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1340",
    doi = "10.18653/v1/P19-1340",
    pages = "3499--3505",
}

@inproceedings{kitaev2018seedparser,
    title = "Constituency Parsing with a Self-Attentive Encoder",
    author = "Kitaev, Nikita  and
      Klein, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1249",
    doi = "10.18653/v1/P18-1249",
    pages = "2676--2686",
}

@inproceedings{pei2017deepxplore,
	doi = {10.1145/3132747.3132785},
	url = {https://doi.org/10.1145%2F3132747.3132785},
	year = 2017,
	month = {oct},
	publisher = {{ACM}},
	author = {Kexin Pei and Yinzhi Cao and Junfeng Yang and Suman Jana},
	title = {{DeepXplore}}, 
	booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles}
}