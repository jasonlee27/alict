\begin{abstract}
  \Nlp (NLP) technique has become one of the core techniques for
  developing text  analytic applications.  These applications need to achieve high
  reliability to be useful in practice.  The trustworthiness of the NLP applications is often obtained by measuring the accuracy of
  the applications on holdout datasets. However, evaluating NLP on the accuracy of the holdout datasets is limited in validating its overall
  quality because these datasets are often not
  comprehensive. To address this limitation, evaluating an NLP model on
  task-specific behaviors defined on linguistic
  capabilities has been introduced. However, such evaluation relies on manually
  created test cases, and  is still limited to measure the model
  performance on limited datasets. In this work, we introduce \tool,
  an NLP model testing infrastructure.  Given a linguistic capability that users want to evaluate for a NLP model, \tool finds suitable seed inputs
  from existing datasets, generates sufficient number
  of new test inputs by expanding the seed inputs based on their \cfg
  (CFG).  We evaluate \tool by showing that it can generate test cases that are more diverse than an existing approach.
  We also show that \tool facilitates
  identification of critical failures and its origins in the NLP models
  for the \sa task.
\end{abstract}

% \begin{abstract}
%   \Nlp (NLP) technique\jl{technique, model, application?} becomes one of the core techniques for developing
%   text analytics applications.  For developing the NLP applications,
%   the applications are required to achieve high reliability before they
%   go to market\jl{<-simplify the sentence}.  The trustworthiness of the prevalent NLP
%   applications is obtained by measuring the accuracy of the
%   applications on test dataset.  However, evaluating NLP on
%   testset \jl{does with held-out accuracy??} is limited to show its quality
%   because the test datasets are often not comprehensive \jl{what does it mean?}. While the behavioral testing over multiple general linguistic capabilities are
%   employed, it relies on manually created test cases, and is still
%   limited to measure its comprehensive performance for each linguistic
%   capability \jl{does not make sense in terms of story telling}. In this work, we introduce \tool, an NLP model
%   testing methodology. Given a linguistic capability, The
%   \tool finds relevant testcases to test the linguistic
%   capability from existing datasets as seed inputs, generates
%   sufficient number of new test cases by fuzzing the seed inputs based
%   on their \cfg (CFG) \jl{<-long sentence}. We illustrate the usefulness of the \tool by showing input diversity and identifying
%   critical failures in \sota models for NLP task. In our experiment,
%   we show that the \tool generates {} more test cases with {}
%   higher diversity, and finds {} more bugs. \jl{describe what lc is about in a sentence} \jl{need to mention the result with numeric values}
  
% \end{abstract}
