\begin{abstract}
  \Nlp (NLP) technique becomes one of the core techniques for developing
  text analytics applications.  For developing the NLP applications,
  the applications are required to achieve high reliability before it
  goes to market.  The trustworthiness of the prevalent NLP
  applications is obtained by measuring the accuracy of the
  applications on held-out dataset.  However, evaluating NLP on
  testset does with held-out accuracy is limited to show its quality
  because the held-out datasets are often not comprehensive. While the
  behavioral testing over multiple general linguistic capabilities are
  employed, it relies on manually created test cases, and is still
  limited to measure its comprehensive performance for each linguistic
  capability. In this work, we introduce \Model, an NLP model
  testing methodology. Given a linguistic capability, The
  \Model finds relevant testcases to test the linguistic
  capability from existing datasets as seed inputs, generates
  sufficient number of new test cases by fuzzing the seed inputs based
  on their context-free grammar (\Cfg). We illustrate the usefulness
  of the \Model by showing input diversity and identifying
  critical failures in \sota models for NLP task. In our experiment,
  we show that the \Model generates {} more test cases with {}
  higher diversity, and finds {} more bugs.  
\end{abstract}
