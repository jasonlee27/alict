\begin{abstract}
  Evaluating natural language model (\Nlp) on testset does with
  held-out accuracy does not explain diverse aspects of language
  capabilities, and results in overrating performace of the \Nlp
  model. In addition, it is not trivial to generate natural language
  inputs for each linguistic capability, and 
  it is even hard to measure the comprehensive
  performance of the \Nlp model for each linguistic capability because
  it is not trivial to generate natural language inputs 
  
  
  
\end{abstract}


Although measuring held-out accuracy has been the primary approach to
evaluate generalization, it often overestimates the performance of NLP
models, while alternative approaches for evaluating models either
focus on individual tasks or on specific behaviors. Inspired by
principles of behavioral testing in software engineering, we introduce
CheckList, a taskagnostic methodology for testing NLP
models. CheckList includes a matrix of general linguistic capabilities
and test types that facilitate comprehensive test ideation, as well as
a software tool to generate a large and diverse number of test cases
quickly. We illustrate the utility of CheckList with tests for three
tasks, identifying critical failures in both commercial and
state-of-art models. In a user study, a team responsible for a
commercial sentiment analysis model found new and actionable bugs in
an extensively tested model. In another user study, NLP practitioners
with CheckList created twice as many tests, and found almost three
times as many bugs as users without it.
