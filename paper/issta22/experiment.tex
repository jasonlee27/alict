\section{Experiment}
\label{sec:experiment}
%
In this section, we present experiments to
evaluate the effectiveness of our proposed evaluation methodology. In
particular, we address the following research questions:

\begin{enumerate}[label=\textbf{RQ\arabic*}]
\item \label{rq:one}: How effective is our proposed evaluation model
  for finding failures given a linguistic capability?
\item \label{rq:two}: How effective is our proposed model for
  generating diverse test cases? % self bleu score
\item \label{rq:three}: How effective is test cases generated from our
  proposed model for detecting diverse type of errors? % retraining
  acc score
\item \label{rq:four}: How effective is our new test case generation
  using \cfg expansion? % ablation study
\end{enumerate}

For answering \ref{rq:one} and \ref{rq:two}, we generate test cases
and use them for evaluating model on linguistic capabilities. In this
experiment, We assess the ability to find failures by anlyzing model's
performance on the generated test cases. We also measure the diversity
among the generated test cases using similarites among them. Next, we
answer \ref{rq:three} by retraining \sa model with generated test
cases and measuring performances. The idea behind this is that more
comprehensive inputs becomes closer to real-world distribution and
addresses more type of errors.  Therefore, it leads to improve the
model performance. In this experiment, We retrain the model and
compare performances of the retrained model. Not only that, we conduct
ablation study of \cfg expansion to understand the its impact in our
approach.

\subsection{Experiment Setup}
%
\MyPara{Seed Input Selection}
%
For each linguistic capability, we first search all sentences that
meet its requirement. Among found sentences, we randomly select 10
sentences due to memory constraint.

\MyPara{Word Sentiment}
%
we extract sentiments of words using the
\Swn~\cite{baccianella2010sentiwordnet}. The \Swn is a publicly
available lexical resource of wrods on Wordnet with three numerical
scores of objectivity, positivity and negativity. Sentiment word
labels from the scores are classified from the algorithm from Mihaela
\etal~\cite{mihaela2017sentiwordnetlabel}.

\MyPara{\Cfg Expansion}
%
We build a reference \Cfg of natural language from the Englush Penn
\Trb corpora~\cite{mitchell1993treebank,nltkTreebankCorporaWebPage}.
The corpus is sampled from 2,499 stories from a tree year \Wsj
collection The \Trb provides a parsed text corpus with annotation of
syntactic and semantic structure. In this experiment We implement the
\trb corpora available through \Nltk, which is a suite of libraries
and programs for \Nlp for english. In addition, we parse the seed
input using into its CFG using the Berkeley Neural
Parser~\cite{kitaev2018constituency, kitaev2019multilingual}, a
high-accuracy parser with models for 11 languages. The input is a raw
text in natural languge and the output is the string representation of
parse tree. Next after comparing CFGs beteen reference and seed input,
we randomly select 10 expansions for generating templates due to
memory constraint.

\MyPara{Synonyms}
%
\Model searches synosyms of each token from synonym sets extracted
from \Wrdnt using \Spacy open-source library for NLP.

\MyPara{Models}
%
We evaluate the following \sa nmodels via \Model:
\Bert~\cite{devlin2019bert}, \Roberta~\cite{liu2019roberta} and
\Dbert~\cite{sanh2019distilbert}. These models are fineturend on \Sstt
and ther accuracies are \BertAcc, \RobertaAcc and \DbertAcc.

\MyPara{Retraining}
%
We retrain \sa models. we split \Model generated test cases into
train/validation/test sets with the ratio of 8:1:1. The number of
epochs and batch_size for retraining are 1 and 16 respectively.

%% For \sa task, input requirements are given for each linguistic
%% capability. Based on the requirements, \Model generates templates and
%% lexicons.
%% \MyPara{Model}
%% \Bert, \Roberta, \Dbert

%% \MyPara{Dataset}
%% reference CFG generation: Treebank corpus
%% word sentiments: sentiwordnet (https://github.com/aesuli/SentiWordNet)
%% cfg parser: berkeley neural parser (https://github.com/nikitakit/self-attentive-parser)
%% synonyms: spacy wordnet synsets (https://spacy.io/universe/project/spacy-wordnet)
%% seed search dataset: stanford sentiment treebank (sst) dataset (https://nlp.stanford.edu/sentiment/)

%% \MyPara{Hyper-Parameter}
%% Number of seed inputs: 10 (random sample)
%% Number of cfg expansions: 10 (random sample)
%% Number of synonyms for a word: 10 (random sample)

%% Besides, we retrain the \sota \sa models aformentioned from test cases
%% generated from \Chlst and \Model. The goal of the retraining is to
%% assess comprehensiveness of test cases according to performance of
%% fine-tuned model. We assume that the comprehensive test cases improve
%% performance of \sa task when fine-tuning with them. On the other hand,
%% fine-tuning models with less comprehensive test cases increases
%% overfitting and decrece model performance.

%% \MyPara{Dataset}
%% Retraining dataset: checklist and our proposed model test cases

%% \MyPara{Hyper-Parameter}
%% Retraining epoach: 1
%% Retraining batch_size: 16
