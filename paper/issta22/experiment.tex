\section{Experiment}
\label{sec:experiment}
%
In this section, we present experiments to
evaluate the effectiveness of our proposed evaluation methodology. In
particular, we address the following research questions:

\begin{enumerate}[label=\textbf{RQ\arabic*}]
\item \label{rq:one}: How effective is our proposed evaluation model
  for finding failures given a linguistic capability?
\item \label{rq:two}: How effective is our proposed model for
  generating diverse test cases? % self bleu score
\item \label{rq:three}: How effective is our proposed model for
  detecting diverse type of errors? % retraining acc score
\item \label{rq:four}: How effective is our new test case generation
  using \cfg expansion? % ablation study
\end{enumerate}

\subsection{Experiment Setup}

For answering \ref{rq:one} and \ref{rq:two}, we organize the
experiment as follows: we generate test cases used for model
evaluation.

\MyPara{Seed Input Selection.} For each linguistic capability, we
search all sentences that meet its requirement. Among found sentences,
we randomly select 10 sentences due to space constraint.

\MyPara{\Cfg Expansion.}

\MyPara{Models.}

In addition, we answer \ref{rq:four} by organizing the another
experiment as follows:

%% For \sa task, input requirements are given for each linguistic
%% capability. Based on the requirements, \Model generates templates and
%% lexicons.
%% \MyPara{Model}
%% \Bert, \Roberta, \Dbert

%% \MyPara{Dataset}
%% reference CFG generation: Treebank corpus
%% word sentiments: sentiwordnet (https://github.com/aesuli/SentiWordNet)
%% cfg parser: berkeley neural parser (https://github.com/nikitakit/self-attentive-parser)
%% synonyms: spacy wordnet synsets (https://spacy.io/universe/project/spacy-wordnet)
%% seed search dataset: stanford sentiment treebank (sst) dataset (https://nlp.stanford.edu/sentiment/)

%% \MyPara{Hyper-Parameter}
%% Number of seed inputs: 10 (random sample)
%% Number of cfg expansions: 10 (random sample)
%% Number of synonyms for a word: 10 (random sample)

%% Besides, we retrain the \sota \sa models aformentioned from test cases
%% generated from \Chlst and \Model. The goal of the retraining is to
%% assess comprehensiveness of test cases according to performance of
%% fine-tuned model. We assume that the comprehensive test cases improve
%% performance of \sa task when fine-tuning with them. On the other hand,
%% fine-tuning models with less comprehensive test cases increases
%% overfitting and decrece model performance.

%% \MyPara{Dataset}
%% Retraining dataset: checklist and our proposed model test cases

%% \MyPara{Hyper-Parameter}
%% Retraining epoach: 1
%% Retraining batch_size: 16
