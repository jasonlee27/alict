@String{AAAI = "AAAI Conference on Artificial Intelligence"}
@String{ACL = "Annual Meeting of the Association for Computational Linguistics"}
@String{AOSD = "International Conference on Aspect-Oriented Software Development"}
@String{ASE = "Automated Software Engineering"}
@String{ASETool = "Automated Software Engineering, Tool Demonstrations"}
@String{CADE = "International Conference on Automated Deduction"}
@String{CAV = "International Conference on Computer Aided Verification"}
@String{CICLing = "International Conference on Intelligent Text Processing and Computational Linguistics"}
@String{CICM = "International Conference on Intelligent Computer Mathematics"}
@String{CICMWIP = "Work in Progress at the Conference on Intelligent Computer Mathematics"}
@String{COLING = "International Conference on Computational Linguistics"}
@String{COQPL = "International Workshop on Coq for Programming Languages"}
@String{CPP = "Certified Programs and Proofs"}
@String{CSUR = "ACM Computing Surveys"}
@String{EACL = "Conference of the European Chapter of the Association for Computational Linguistics"}
@String{ECOOP = "European Conference on Object-Oriented Programming"}
@String{EMNLP = "Empirical Methods in Natural Language Processing"}
@String{ESEC/FSE = "Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering"}
@String{ESOP = "European Symposium on Programming"}
@String{FASE = "Fundamental Approaches to Software Engineering"}
@String{FM = "International Symposium on Formal Methods"}
@String{FSE = "International Symposium on the Foundations of Software Engineering"}
@String{FSENIER = "International Symposium on the Foundations of Software Engineering, NIER"}
@String{ICLR = "International Conference on Learning Representations"}
@String{ICML = "International Conference on Machine Learning"}
@String{ICPC = "International Conference on Program Comprehension"}
@String{ICSE = "International Conference on Software Engineering"}
@String{ICSME = "International Conference on Software Maintenance and Evolution"}
@String{ICST = "International Conference on Software Testing, Verification, and Validation"}
@String{IJCAI = "International Joint Conference on Artificial Intelligence"}
@String{IJCAR = "International Joint Conference on Automated Reasoning"}
@String{ISSTA = "International Symposium on Software Testing and Analysis"}
@String{ITP = "International Conference on Interactive Theorem Proving"}
@String{LPAR = "International Conference on Logic for Programming, Artificial Intelligence, and Reasoning"}
@String{MSR = "International Conference on Mining Software Repositories"}
@String{NAACL = "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}
@String{NeurIPS = "Annual Conference on Neural Information Processing Systems"}
@String{NIPS = "Annual Conference on Neural Information Processing Systems"}
@String{OOPSLA = "International Conference on Object-Oriented Programming, Systems, Languages, and Applications"}
@String{PLDI = "Conference on Programming Language Design and Implementation"}
@String{POPL = "Symposium on Principles of Programming Languages"}
@String{SANER = "International Conference on Software Analysis, Evolution and Reengineering"}
@String{SOOPPA = "Symposium on Object-Oriented Programming Emphasizing Practical Applications"}
@String{SOSP = "Symposium on Operating Systems Principles"}
@String{TPHOLs = "Theorem Proving in Higher Order Logics"}
@String{TYPES = "International Conference on Types for Proofs and Programs"}
@String{UITP = "International Workshop On User Interfaces for Theorem Provers"}
@String{WCRE = "Working Conference on Reverse Engineering"}


@InProceedings{recht2019imagenetbias,
  title = 	 {Do {I}mage{N}et Classifiers Generalize to {I}mage{N}et?},
  author =       {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5389--5400},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/recht19a/recht19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/recht19a.html},
  abstract = 	 {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the modelsâ€™ inability to generalize to slightly "harder" images than those found in the original test sets.}
}

@inproceedings{wu2019errudite,
    title = "{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",
    author = "Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1073",
    doi = "10.18653/v1/P19-1073",
    pages = "747--763",
    abstract = "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",
}

@inproceedings{marcoACL2020checklist},  
 author = {Marco Tulio Ribeiro and Tongshuang Wu and Carlos Guestrin and Sameer Singh},  
 title = {Beyond Accuracy: Behavioral Testing of NLP models with CheckList},  
 booktitle = {Association for Computational Linguistics (ACL)},  
 year = {2020}  