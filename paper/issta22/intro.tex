\section{Introduction}
\label{sec:intro}

Software testing is the cruicial process when developing software.  It
evaluates an attribute or capability of the software and determines
that it meets the requirements by examining the behavior of the
software under test. Software testing in the early stage of the
development finds bugs, and fixing them saves amount of costs. In
addition, reliable software testing methodology ensures software
quality to users in that the software meets requirements by
verification and validation. Regarding that, NLP application is a
branch of artifical intelligence software, and testing NLP application
also becomes important process as well.

The prevalent models of NLP are evaluated via train-validation-test
splits. train and validation set is used to train the NLP model and
the \ho set is used for testing by measuring accuracy. The accuracy is
a indicator of the performance of the models. Despite its usefulness,
the main limitation of the testing paradigm is that the \ho set often
overestimates the performances. Each dataset comes with specific
biases, and the biases increase the discrepancy of distribution
between dataset and real-world~\cite{recht2019imagenetbias}. The
aforementioned accuracy on \ho set does not consider the discrepancy
and it is limited to achieve comprehensive performrance of the NLP
model. As a consequence, it is difficult to analyze weekness of the
model~\cite{wu2019errudite}.

% this paragraph goes to related, and saying behavioral testing and
% its limitation based on our model's novelty
On the subject of the limitation of traditional testing paradaigm, a
number of methods have been proposed. First, multiple diagnostic
datasets for evaluating NLP model were introduced for obtaning
generalized evaluation of the NLP model~\cite{wang2018glue}. Not only
that, model is evaluated on different aspects such as robustness of
the model on adversarial
sets~\cite{ribeiro2018sear,belinkov2018breaknmt,
  rychalska2019wildnlp,iyyer2018adversarial},
fairness~\cite{prabhakaran2019fairness,rottger2020hatecheck}, logical
consistancy~\cite{ribeiro2019consistencyeval}, prediction
interpretations~\cite{ribeiroSG16lime} and interactive error
analysis~\cite{wu2019errudite}. Especially, \Chlst implements
behavioral testing methodolgy for evaluating multiple linguistic
capabilities of NLP model~\cite{marcoACL2020checklist}. \Chlst
introduces input-output behaviors of linguistic capabilities and
generates behavior-guided inputs for validating the behaviors. It
provides comprehensive behavioral testing of NLP models through a
number of generated inputs. However, the approach only relies on
manually generated input templates, thus the template generation
becomes expensive and time consuming. In addition, the generated
templates are selective and often too simple, and it is limited to
provide restricted evaluation of linguistic capabilities. Thus, it
does not garauntee the comprehensive evaluation.

In this paper, we present \Model, an automated NLP model evaluation
method for comprehensive behavioral testing of NLP models on \sa
task. For each behavior of linguistic capability, \Model does not rely
on the manual input generation. Instead, it establishes input
requirement for evaluating a linguistic capability and finds suitable
inputs that meet the requirement from existing public dataset.
Therefore, \Model increases input diversity and generality. Further,
\Model applies the fuzzing testing principle to generate inputs by
mutating the selected inputs as seed inputs. Fuzzer in \Model first
expands seed input grammar structures and determines its available
\pos to maintain structural naturalness. After that, to hold
contextual naturalness of the mutated inputs, the fuzzer completes the
expanded new structures via data-driven context-aware word
suggestion. Additionally, sentiment-independent words in the inputs
are replaced with rule-based word suggestion.

We demonstrate its generality and utility as a NLP model evaluation
tool by evaluating well-known \sa models: \Bert~\cite{devlin2019bert},
\Roberta~\cite{liu2019roberta} and \Dbert~\cite{sanh2019distilbert}.
We show that


% our paper's contribution
