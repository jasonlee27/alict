\section{Introduction}
\label{sec:intro}

Software testing is the cruicial process when developing software.  It
evaluates an attribute or capability of the software and determines
that it meets the requirements by examining the behavior of the
software under test. Software testing in the early stage of the
development finds bugs, and fixing them saves amount of costs. In
addition, reliable software testing methodology ensures software
quality to users in that the software meets requirements by
verification and validation. Regarding that, testing NLP application
becomes important process as well in that NLP application is a branch
of artifical intelligence software.

Prevalent NLP model is evaluated via train-validation-test
splits. train and validation set is used to train the NLP model and
the \ho set is used for testing by measuring accuracy 

testing \ml becomes major 

\Nlp testing is crucial part for developing a reliable \Nlp
applications. For developing \Nlp applications, testing is mainly used
to check the ML model's performance on \ho set with respect to the
accuracy of the model. The \ho data refers to a portion of dataset
that is held out of the datasets used for training \ml
models. Generally, the \ho set is extracted via Train-Validation-Test
split.

%% One of the primary goals of training NLP models is
%% generalization. Since testing “in the wild” is expensive and does not
%% allow for fast iterations, the standard paradigm for evaluation is
%% using trainvalidation-test splits to estimate the accuracy of the
%% model, including the use of leader boards to track progress on a task
%% (Rajpurkar et al., 2016).

%% While performance on held-out data is a
%% useful indicator, held-out datasets are often not comprehensive, and
%% contain the same biases as the training data (Rajpurkar et al., 2018),
%% such that real-world performance may be overestimated (Patel et al.,
%% 2008; Recht et al., 2019). Further, by summarizing the performance as
%% a single aggregate statistic, it becomes difficult to figure out where
%% the model is failing, and how to fix it (Wu et al., 2019).
