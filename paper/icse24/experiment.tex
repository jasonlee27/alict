\section{Experimental Setup}
\label{sec:experiment}


In this section, we present the setup of our experiments. We answer the following research questions (RQs):

\begin{enumerate}[label=\textbf{RQ\arabic*}]
    \item \label{rq:one} \textbf{Diversity.} Can \tool generate more diverse test cases than existing approaches?
    
    \item \label{rq:two} \textbf{Effectiveness.} Is \tool more effective than existing approaches at generating test cases that can trigger more errors in the model?
    
    
    
    \item \label{rq:three}  \textbf{Consistency.} Can \tool maintain consistency in terms of labels, linguistic capabilities, and semantics throughout the expansion process?
    % \TODO{CM: what is correct labels}
    
    % \item \label{rq:four}  \textbf{Capability Consistency.} Are test cases generated by \tool correctly categorized into a linguistic capability?
    
\end{enumerate}

%To answer the RQs, we need to show: (\romnum{1}) correctness of testcase to
%evaluate its target \lc.  (\romnum{2}) effect of testcase distribution on
%finding bugs; (\romnum{3}) degree of execution of a \sa model on testcases;
%and (\romnum{4}) ability to guide to find cause of bug in a \sa
%model.

\input{tables/model}

\subsection{Experimental Subjects}
\MyPara{NLP Models} We evaluate our approach on three \sa models and two \hsd models.  We obtain our evaluation models from the HuggingFace model hub \cite{huggingface}.
Table \ref{tab:model} presents the models and their corresponding API URLs. The ``API URL'' column displays the public URL of each model, while the ``$\#$ of downloads'' column indicates the number of downloads for each model in Jan. 2023. Based on the information provided in Table \ref{tab:model}, it is evident that all models utilized in our evaluation have been widely adopted in real-world settings, with a considerable number of downloads. 
For readers who seek more comprehensive information about each model, we recommend referring to the corresponding API URL of each model.

%\CM{do we need to create a table to show these models are public API and are widely used} \footnote{https://huggingface.co/models} \texttt{\Bert}
%(\bertsamodel), \texttt{\Roberta} (\robertasamodel), and \texttt{\Dbert} (\disbertsamodel~\cite{sanh2019distilbertad}) for \sa, and \texttt{\hsdBert} (\berthsdmodel~\cite{aluru2020deep}) and \texttt{\hsdRoBerta} (\robertahsdmodel) for \hsd.
%These models were pre-trained on English language with a masked language modeling objective, and were fine-tuned on the \sa and \hsd tasks.

\MyPara{Datasets} In our evaluation of sentiment analysis models, we utilize \Sst~\cite{socher2013sst} as our initial labeled search dataset. Meanwhile, for assessing models designed for hate speech detection, we make use of \Hatexp~\cite{mathew2021hatexplain} as our initial dataset.
\Sst is a corpus of movie reviews that consists of 11,855 sentences, each of which has been labeled as either negative, neutral, or positive to indicate the expressed sentiment in the sentence.
% Following the original setting of \Sst~\cite{socher2013sst}, we divide the scores into ranges [0, 0.4], (0.4, 0.6] and (0.6, 1.0] to assign them negative, neutral and positive labels, respectively.
\Hatexp is a dataset that has been collected from social media platforms Twitter and Gab. It consists of 20,148 sentences, with 9,055 of them being from Twitter and 11,093 from Gab. Each sentence in this dataset has been labeled as either "hate" or "non-hate" to indicate the presence or absence of hate speech in the sentence \cite{mathew2021hatexplain}.


% has been annotated by three different annotators.
% For each sentence, it has been labeled as ``hate'' label when majority label over the annotators is hate, and non-hate label otherwise.
% \sw{I don't understand what ``three labels of hate speech for three different annotators" mean. Explain to me and we will update.}

% as shown in Figure \ref{fig:overview}.

\MyPara{\Bls}
\label{sec:experiment_bl}
For RQ1, we first compare \tool against \Cklst~\cite{marcoACL2020checklist} and \hck~\cite{rottger2020hatecheck}. After that, we also compare \tool against \mtnlp, Alzantot-attack, BERT-Attack and SememePSO-attack~\cite{maIJCAI2020mtnlp,alzantot2018genadvexp,linyang2020bertattack,zang2020sememepso}.
For RQ2, we compare \tool against \Cklst and \hck again.

% \TODO{Notice that although there are several existing adversarial NLP techniques that can generate test inputs to test NLP models, 
% We compare \tool against 
% \Cklst and \hck are manual template- and \lc-based test case generation approaches for \sa and \hsd, respectively. 
% In this evaluation, we used \Cklst's \sa test cases that are generated from its original implementation \footnote{https://github.com/marcotcr/checklist} and \hsd test cases on the \hck's public 
% Github pages.\footnote{https://github.com/paul-rottger/hatecheck-data}
% by comparing them with \tool generated test cases and \tool expanded test cases from \Cklst and \hck as seeds.
% We also consider existing text generation methods to evaluate \tool's syntax-based expansion phase. First, we compared to \mtnlp, another syntax-based text generation tool. While there exist other syntax-based other syntax-based text generation methods \cite{udeshi2019ogma,soremekun2022astraea,asyrofi2021biasfinder}, their tools are not publicly available or we were not successfully build them. \sw{Is the last \sent (which I changed) correct? Need to be careful.} \mtnlp presents a NLP fuzz testing approach to evaluate NLP model 
% fairness by replacing a word with its analogy or inserting fairness-sensitive words.
% % \sw{Resume here.}
% Second, we compared to three adversarial example generation methods~\cite{alzantot2018genadvexp,linyang2020bertattack,zang2020sememepso}. \sw{Is it right that we are comparing to three? What is the name for each?} Alzantot~\etal attacks a NLP model with nearest neighbors of input word in embedding space~\cite{alzantot2018genadvexp}. Bert-Attack finds adversarial words in \sent using suggestions from a pretrained masked language model~\cite{linyang2020bertattack}. SememePSO-Attack substitutes word using \sw{What is ``particle swarm optimization" and what is ``search space"?} particle swarm optimization method in search space~\cite{zang2020sememepso}.
% }
% A search algorithm based on particle swarm
% optimization is proposed to search adversarial
% examples.


\begin{equation}
\label{eq:syntactic}
    Syntactic\;Diversity(\mathcal{X}) = || \{\mathcal{P}(x) \;\; | \;\; \forall x \in \mathcal{X} \}||
\end{equation}

\subsection{Evaluation Metrics}
\label{sec:metric}
To answer RQ1, we define three metrics to measure the diversity of the generated test suite. Our first metric is \textit{\selfbleu}, which was introduced by \cite{zhu2018texygen}. \selfbleu is defined as the average \bleu score~\cite{papineni2002bleu} over all reference sentences, ranging between 0 and 1. A higher \selfbleu score indicates lower diversity in the test suite, while a lower score indicates greater diversity.
However, since \selfbleu cannot represent the syntactic diversity of a test suite, we have proposed a new metric, \textit{syntactic diversity}, to evaluate the diversity of the test suite. The syntactic diversity of a test suite $\mathcal{X}$ is defined as the number of production rules covered in this test suite. The formal definition of syntactic diversity is shown in  \eqref{eq:syntactic}, where $\mathcal{P}$ is the Berkeley Neural Parsing function~\cite{kitaev2018seedparser,kitaev2019seedparser} that return the production rule of the given sentence.
Our final metric is \textit{neuron coverage}. 
We follow the approach presented by Ma et al. \cite{ma2018deepgauge}, where the authors measure the coverage of NLP model intermediate states as corner-case neurons.
Because the matrix computation of intermediate states impacts NLP model decision-making, a test suite that covers a greater number of intermediate states can represent more NLP model decision-making, making it more diverse.
Specifically, we used two coverage metrics by Ma et al. \cite{ma2018deepgauge}, boundary coverage (BoundCov) and strong activation coverage (SActCov), to evaluate the test suite diversity.
% It is worth noting that a test sample with a statistical distribution similar to the training data is rarely found in the corner case region.
% Thus, covering a larger corner case region indicates that the test suite is more likely to be buggy.
\begin{equation}
\begin{split}
    \text{UpperCorner}(\mathcal{X}) = \{n \in N | \exists x \in \mathcal{X}: f_n(x) \in (high_n, +\infty)\}; \\
    \text{LowerCorner}(\mathcal{X}) = \{n \in N | \exists x \in \mathcal{X}: f_n(x) \in (-\infty, low_n)\}; \\
\end{split}
    \label{eq:corner}
\end{equation}

\noindent Equation \ref{eq:corner} defines the corner-case neuron of the NLP model $f(\cdot)$, where $\mathcal{X}$ is the given test suite, $N$ is the number of neurons in model $f(\cdot)$, $f_n(\cdot)$ is the $n^{th}$ neuron's output, and $high_n$ and $low_n$ are the $n^{th}$ neurons' upper and lower output bounds on the model training dataset respectively.
Equation \ref{eq:corner} can be interpreted as the collection of neurons that emit outputs beyond the model's numerical boundary.

\begin{small}
\begin{equation}
\begin{split}
     & BoundCov(\mathcal{X}) = \frac{|UpperCorner(\mathcal{X})| + |LowerCorner(\mathcal{X})| }{2 \times |N|} \\ 
     &\quad  \qquad \qquad  SActCov(\mathcal{X}) = \frac{|UpperCorner(\mathcal{X})|} {|N|} \\ 
\end{split}
    \label{eq:coverage}
\end{equation}
\end{small}

\noindent The definition of our model coverage metrics is shown in Equation \ref{eq:coverage}, where BoundCov measures the coverage of neurons that produces outputs exceeding the upper or lower bounds, and SActCov measures the coverage of neurons that creates outputs exceeding the lower bound.
Higher coverage indicates the test suite is better for triggering the corner-case neurons, thus better diversity. 



For RQ2, our goal is to answer whether \tool is more effective than other methods for generating test cases that can trigger incorrect predictions. Thus, we measure three key metrics: (1) the number of test cases generated, (2) the number of failed test cases, and (3) the failure rates of the generated test cases. Additionally, we also report on the number of expanded test cases that failed but whose corresponding seed test cases passed (\Ptf) using \tool.



\begin{equation}
\label{eq:consistency}
\begin{split}
    &  LabelCons = \frac{1}{\#Sample}\cdot \sum_{i}\delta(label_{S^2LCT}=label_{human}) \\
    & LCRel_{AVG} = \frac{1}{\#Sample}\cdot\sum_{i} Norm(LCRel_i)   \\
    &  ExpValidity_{AVG} = \frac{1}{\#ExpSample}\cdot\sum_{i} Norm(ExpValidity_i)  
\end{split}
\end{equation}

% \begin{equation}
% \begin{small}
% \begin{aligned}
%   LabelCons = \frac{1}{\#Sample}\cdot \sum_{i}\delta(label_{S^2LCT}=label_{human})\label{eq:lcons}
% \end{aligned}
% \end{small}
% \end{equation}
% \begin{equation}
% \begin{small}
% \begin{aligned}
%   LCRel_{AVG} = \frac{1}{\#Sample}\cdot\sum_{i} Norm(LCRel_i) \label{eq:lcrel}
% \end{aligned}
% \end{small}
% \end{equation}
% \begin{equation}
% \begin{small}
% \begin{aligned}
%   ExpValidity_{AVG} = \frac{1}{\#ExpSample}\cdot\sum_{i} Norm(ExpValidity_i) \label{eq:expval}
% \end{aligned}
% \end{small}
% \end{equation}
 

% \begin{equation}
% \label{eq:consistency}
%     \begin{split}
%         & \qquad\quad \zeta_{label} = \frac{1}{||\mathcal{X}||} \sum_{x \in \mathcal{X}} \mathbb{I}(label(x) == \text{HLabel(x)}) \\
%         & \zeta_{linguistic} = \frac{1}{||\mathcal{X}||} \sum_{x \in \mathcal{X}} \mathbb{I}(linguistic(x) == \text{Hlinguistic(x)}) \\
%         & \zeta_{semantic} = \frac{1}{||\mathcal{X}||} \sum_{x \in \mathcal{X}} \mathbb{I}(\text{HSemantic}(x) == \text{HSemantic}(Seed(x)))  \\
%     \end{split}
% \end{equation}


To answer RQ3, we introduce three new metrics: the label consistent rate ($LabelCons$), the linguistic capability consistent rate ($LCRel_{AVG}$), and the semantic consistent rate ($ExpValidity_{AVG}$). The formal definitions of these metrics are listed in Equation \ref{eq:consistency}.
% In Equation \ref{eq:consistency}, HLabel$(\cdot)$, Hlinguistic$(\cdot)$, and HSemantic$(\cdot)$ represent the label, linguistic capability, and semantic assigned by humans to a given sentence, while $label(\cdot)$ and $linguistic(\cdot)$ are functions used to obtain the automatic constructed label, linguistic capability of a test input.
 
% \TODO{Equation~\ref{metric:srel} represents the percentage of the test cases that \tool and the participants 
% produce the same sentiment labels. High value of this metric indicates \tool generates test cases with
% correct labels. Equation~\ref{metric:lcrel} represents the average of the
% normalized relevancy score between a \sent and its associated
% \lc. The relevancy score is to evaluate the relevancy of the test case on the 
% corresponding \lc. Higher average score indicates the linguistic capability 
% categorization by \tool is correct. 
% Equation~\ref{metric:expval} represents expansion validity, the average of the normalized validity score between expanded \sent and its corresponding seed \sent.
% The higher score indicates higher semantic similarity between expanded \sent and its corresponding seed \sent enough to use semantic label of the seed \sent for the expanded \sent.
% We answer \ref{rq:three} and \ref{rq:four} using the metrics defined by
% Equations~\ref{metric:srel}, \ref{metric:lcrel} and~\ref{metric:expval}, respectively.}


% In our experiments, we used the Berkeley Neural Parser~\cite{kitaev2018seedparser,kitaev2019seedparser} to parse and
% collect all the productions.

% First, we reuse the input diversity metric,
% called , that Zhu \etal introduced~\cite{zhu2018texygen}. \bleu~\cite{papineni2002bleu} evaluates token-level similarity.
% Regarding that \selfbleu takes each \sent as hypothesis and rest in a collection of textual data as reference, we calculate \bleu scores for every pairs of hypothesis and each reference \sent.
% Since the \bleu score ranges from 0 as the least similar inputs to 1 as the most similar inputs,
% A higher \selfbleu score indicates lower diversity in the test suite.
% Second, we propose a new metric to 
% evaluate the syntactic diversity of the generated test suite.
% It is defined as the number of production rules covered in a set of 
% test \sents. In our experiments, we used the Berkeley Neural 
% Parser~\cite{kitaev2018seedparser,kitaev2019seedparser} to parse and
% collect all the productions.




% To answer RQ2, 



\subsection{Experimental Process}



\MyPara{\ref{rq:one} Process} In the evaluation, we collected 200, 400, 600, 800 and 1000 test cases for \sa and 10000, 50000, 100000, 150000 and 200000 test cases for \hsd, randomly selected \tool seed and expanded \sents. We then computed the median of \selfbleu and \pdr scores over all \lcs. We repeated this computation with different \tool seeds over 5 trials and reported the median. 
% \sw{I am lost with the rest of this paragraph. Explain to me; then we will fix.}
We also evaluated \tool expansion phase by generating expanded \sents from \Cklst and 
\hck as seeds. We collected up to 200 randomly selected test cases from \Cklst and 
\hck and generate their expanded \sents. We computed the median of \selfbleu and \pdr 
scores from the \sents over all \lcs. We repeated the computation with different \tool
seeds over 3 trials and reported the median over the 3 trials.
In addition, we compared \selfbleu and \pdr scores between \tool and text generation 
\bls. First, we generate two groups of \sents from 100 randomly selected \tool seeds 
for each \sa and \hsd using \tool expansion and syntax-based text generation \bl. 
\selfbleu and \pdr scores of the two groups of \sents were then compared. Second. we 
generate two groups of \sents from 50 randomly selected \tool seeds for \sa using 
\tool expansion and adversarial text generation \bls. Likewise, we compared \selfbleu 
and \pdr scores of the two groups of \sents.

% \TODO{how about production rule coverage}

For the model coverage metric, we begin by feeding the training dataset of each NLP model under test into the system in order to compute the lower and upper bounds for each neuron. Then, we select an equal number of test cases from both \tool and \Cklst to construct the test suite and calculate the corresponding model coverage metrics.




% Recall that \Cklst relies on significant manual efforts and may not generate comprehensive test cases in a linguistic capability. \tool, instead, automatically generates test cases based on a search dataset and the syntax in a large reference corpus. We expect \tool can generate more qualitative test cases than \bls.




\MyPara{\ref{rq:two} Process}
% We expect that the iterative seed generation and syntactic expansion generate more qualitative and quantitative test suite for finding errors effectively.s
% To evaluate test effectiveness of \tool, we compared \tool and capability-based testing \bls 
% using (1) number of test cases, (2) number of failed test cases, and (3) failure rates of NLP models under test.
% In addition, we report the number of expanded test cases that failed, 
% but their corresponding seed test cases passed (\Ptf) from the \tool.
% \TODO{Recall that our syntax-based expansion validates label consistency; thus, the models are expected to classify the expanded sentences with the same labels as their seed sentences. 
% However, we observed that an expanded sentence might deceive the model into producing a different prediction from its seed. Identification of these cases may provide further insight into the root causes of the erroneous behaviors of the model in the \lc, which we demonstrate in an application of \tool (Section \ref{sec:application}). 
% }
We address RQ2 by evaluating 5 models in Table~\ref{tab:model} on test cases of \tool and \lc-based testing \bls for \sa and \hsd. For each \lc, we measure the number of test cases generated from the \bls, \tool seeds and their expansions. We calculate the number of failures and fail rate of the 5 models. In addition, we compare model performances on test cases between \tool seeds and their expansions, and measure the number of \Ptf cases.
% Accordingly, we compute the median of the number of \Ptf test cases over 
% all \lcs between 200, 400, 600 and 800 randomly selected \tool seeds 
% and the \Cklst test cases, and we compare the median of the median numbers 
% over 10 trials for each group of seeds.

\MyPara{\ref{rq:three} Process}
% As described in Section~\ref{sec:approach},
% \tool generates test cases in two steps: specification-based seed generation and syntax-based \sent expansion. These automated steps
% may generate seed/expanded \sents marked with incorrect sentiment
% labels or categorized into wrong linguistic capabilities.
% For example,
% the search rule and template defined in a linguistic capability may
% not always generate seed \sents in that capability or with the
% correct label. 
To address RQ3, we conduct a manual study to evaluate the three consistency metrics listed in Equation \eqref{eq:consistency} for the test suite generated by \tool. For each task, we randomly sampled 100 \tool seed \sents. We divide these seeds to two sets (i.e., 50 \sents in each set). For each sampled seed \sent, we randomly obtain one of its expanded \sents.
This forms the two sets of \sents, each with 50 seeds and 50 corresponding expanded \sents. We recruited two participants for each task; all of them are graduate students with no 
knowledge about this work. Each of them was assigned a different set of \sents, and asked to provide three scores for each \sent. \emph{(1) Relevancy score between \sent and its associated \lc}:
this score measures the correctness of \tool linguistic capability categorization.  The scores are discrete, ranging from 1 (``strongly not relevant'') to 5 (``strongly relevant'').
\emph{(2) Sentiment score of the \sent}: this score measures the sentiment level of the \sent . It is also discrete, ranging from 1 to 5 representing ``strongly negative'' to ``strongly positive'' for \sa and ``strongly normal'' to ``strongly hateful'' for \hsd, respectively.
\emph{(3) Validity score of expanded \sent}: this score measures the validity of the use of label of a seed \sent for its associated \tool expanded \sent. The scores are discrete ranging from 1 (``strongly not consistent'') to (``strongly consistent'').

% \sw{If possible, make each of the following equations show in one line.}\jl{made them in one line.}

\MyPara{Implementation Details} In our implementation, we obtained our reference CFG from the Penn Treebank corpus~\cite{mitchell1993treebank}. Additionally, we utilized \Swn~\cite{baccianella2010sentiwordnet}, which is a lexical sentiment resource, as the domain-specific knowledge for sentence expansion. All experiments were conducted on a Ubuntu 14.04 server with three Intel Xeon E5-2660 v3 CPUs @2.60GHz, eight Nvidia 1080Ti GPUs, and 500GB of RAM.