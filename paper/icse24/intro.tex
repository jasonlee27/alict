\section{Introduction}
\label{sec:intro}

% In the early stage of the software development process, automated
% software testing and debugging can identify and fix
% defects. Therefore, effective software testing assures software
% quality, and it meets needs of the end user. Nowdays, software at work
% has elements of \ml (ML). Especially, 


\Nlp (NLP) applications are growing rapidly.  As a result, trustworthiness in 
how the quality of NLP applications is assessed becomes critical for its 
practical use in the real world. Traditionally, the quality of NLP models is 
assessed using aggregated metrics. In particular, accuracy (\ie, the fraction of 
outputs that the model correctly predicts) is the most widely used metric for 
assessing the quality of classification models:
higher accuracy suggests better quality of a model. However, all NLP models have 
their strengths and weaknesses; using a single, aggregated metric (i.e., 
accuracy) makes it difficult for the users to assess model behavior on fine-grained aspects of NLP models. To address this limitation of the 
traditional quality metric, an increasing number of evaluation methods has been 
introduced to evaluate an NLP model in different aspects such as robustness on 
adversarial examples~\cite{morris2020textattack,zang2020sememepso,alzantot2018genadvexp,linyang2020bertattack,udeshi2019ogma} and bias on demographic groups~\cite{soremekun2020astraea,udeshi2018fairnesstest,asyrofi2022biasfinder,maIJCAI2020mtnlp}. However, such metrics still assess specific facet of the NLP models, and they still fail to validate how well a model behaves in different linguistic scenarios~\cite{geva2019we,gururangan2018annotation}. 

Recent research has proposed methods to assess an NLP model on a set of \lcs in 
different NLP applications~\cite{marcoACL2020checklist,rottger2020hatecheck}. 
A \lc is a specific linguistic scenario between input and output observed in the  domain 
of NLP application. Compared with the traditional evaluation metrics, 
\lc-based testing avoids the overestimation of the model performance by
measuring variances of model performance over the capabilities. Thus,
it is able to give detail assessment on strength and weakness of a given NLP model.
% \sw{The last sentence is a strong claim. Is there a citation for this claim? Maybe checklist?}
% For example, performance of \sa model on negation and questionization in texts can be different, and variance in the model performances enables identification of certain input type that causes the performance inconsistency in NLP models~\cite{marcoACL2020checklist}.
Nevertheless, existing \lc-based testing approaches are limited to facilitate automated and comprehensive testing.
%rely on unprincipled description of \lcs. In addition,
They rely on manually designed word substitution-based templates. The test cases are generated by  replacing words on the place holders in the templates \cite{marcoACL2020checklist, rottger2020hatecheck} 
% \CM{fix} \sw{cite checklist and hatecheck again here?}. 
Therefore, their test cases  do not cover diverse \sent structures, preventing comprehensive testing of NLP model on \lcs.

To address this issue, we present \tool,
an automated linguistic capability-based framework for testing NLP models. 
% Instead of relying on the word-level templates, we transform test case from the real-world dataset into seeds by perturbing it on search-based and enumerative \phs defined on \lc. Next, \tool finds potentially extensive syntactic elements from the seeds by comparing their production rules with the reference corpus. \tool obtains the appropriate the test sentence and test oracle by implementing domain-specific knowledge on the word sentiment and word suggestion model for validating the generated sentence and its oracle.
The goal of this work is to generate a diverse \lc-based test suite automatically.
% Key challenge to achieve the goal is \textit{Consistency}.
% As the semantics of nature language \sents can be greatly changed even by a slight perturbation to the \sents, it is challenging to maintain consistency of test case to the \lc and its label, enhancing diversity of the test case at the same time. To address this difficulty, \tool defines \emph{specifications} (search-based and enumerative seed generation \phs) from \lcs,
% and generates seed test cases by perturbing test cases into the \phs. In addition, \tool validates coherence of \lc specification and test oracles defined by input-output relations determined by the \lc specification.
To achieve the goal, \tool needs to address two requirements:
\textit{(i) Capability-based Generation.} Each test case should be automatically categorized into a \lc;
and \textit{(ii) Automated Test Oracle.} The label of each test case should be automatically
  and accurately defined.

\noindent \textbf{Capability-based text generation.} Whether a test sentence is suitable for evaluating a specific \lc is decided by its relevancy to this
\lc. It is challenging to maintain relevancy between a test sentence and its \lc when generating the test sentence.
This is because the \lc is defined on a specific 
mixture of syntax and semantics of the sentences. Due to the inherent ambiguity of natural language \sents, there exists no automatic way to check the consistency of each sentence with the semantics specified in the corresponding \lc. Existing metamorphic or adversarial testing~\cite{morris2020textattack,zang2020sememepso,alzantot2018genadvexp,linyang2020bertattack,udeshi2019ogma} considers only labels of generated test cases without linguistic capacity guarantee. To address this difficulty, \tool defines \emph{specifications} (search-based and enumerative seed generation \phs) from \lcs,
and generates seed test cases by perturbing test cases into the \phs. In addition, \tool analyzes the parse trees of the seed sentences to identify possible expansion and validates coherence of \lc relevancy between each seed and its expansion.

\noindent \textbf{Automated Test Oracle.}   For NLP model testing, test oracle is usually determined by understanding the semantics of texts, which requires domain knowledge for different NLP tasks \cite{barr2014oracle, nmt_se1, nmt_se2, nmt_se3}.
Thus, the current testing practice requires manual effort to create the test oracles of the \ho data, which is time-consuming. Therefore, it necessitates automated test oracle generation for improving the testing process of NLP models. 
However, automatically generating the  test oracles remains one of the main challenges in NLP testing~\cite{huang2022aeon, nmt_se1, nmt_se2, nmt_se3}. 
Several metamorphic testing approaches have been proposed in image recognition domain, but they require understanding the characteristics of metamorphic relations between inputs and outputs, which require domain expertise and non-trivial manual efforts ~\cite{tian2018deeptest, ma2018deepgauge, pei2017deepxplore, xie2019deephunter, zhang2018deeproad, chen2022deepperform}. 
% \CM{fix} \sw{The only citation for metamorphic testing? We say there are ``several metamorphic testing approaches"}. 
It is even more challenging to design metamorphic relations in textual data because the semantics of nature language \sents can be greatly changed even by a slight perturbation to the \sents. We address the challenge by only accepting validated test oracles defined by input-output relations determined by the \lc specification. 
% \jl{fix}\sw{The last sentence seems broken (I made a slight change). But I don't understand what the ``determined by input-output relations" part means.}

In this work, as a first step, we consider \sa and \hsd as the NLP applications for automated \lc-based testing.
We demonstrate the effectiveness of \tool by evaluating three \sa and two \hsd models.
% We reveal that addressing these requirements contributes to automated test case generation for \lc testing, measured in terms of label and capability consistency in \sents.

% Experiments show that \tool test cases are 
% % \sw{Not clear how we got the 100\% value. Generally, need to double check all the results here after evaluation is finalized.}\jl{comparing \selfbleu and \pdr scores of \tool and \Cklst and \hck shows the smallest ratio (\tool metric/baseline metric) is 0.88 from \selfbleu} 
% at least 88\% more diverse in \selfbleu and \pdr metrics, and \tool finds more failures than the \sota approaches in 21 out of 24 \lcs over the two NLP applications. 
% In addition, we perform a manual study and confirm that \tool test cases are accurately labelled  and categorized into \lcs. Besides, based on an existing explainable ML technique \cite{simin2020denas}, we perform a case study to show that \tool is useful to support understanding the erroneous model behavior.
% % : \Bert~\cite{devlin2019bert},
% % \Roberta~\cite{liu2019roberta} and \Dbert~\cite{sanh2019distilbert}.


\vspace{2pt}
\noindent
In summary, we made the following contributions in this work:

\begin{itemize}[topsep=3pt,itemsep=3pt,partopsep=0ex,parsep=0ex,leftmargin=*]
% \item We design and instantiate formal specifications of 10 and 20 \lcs on \sa and \hsd tasks respectively.
%
\item We design and implement an 
% \sw{do we want to say ``the first"?} 
automated linguistic capability-based testing approach, \tool, capable of producing diverse test cases while performing capability-based text generation using automated test oracle.
%
\item We evaluate text classification models on test cases generated by \tool on 10 and 14 \lcs for \sa and \hsd, respectively. Comparing with the \sota \lc-based testing baselines, we find that \tool produces at least 88\% more diverse test cases, measured in \selfbleu and \pdr, and a larger number of NLP model failures in 21 out of 24 \lcs over the two NLP applications.
We also analyze the root causes of the misclassifications, and find that seeds and expanded test cases produced by \tool are useful in helping developers understand bugs in the model. 
% \sw{Quite some redundancy in this paragraph with the paragraph before ``We made the following contributions ...". I did not address this because I think we need to know the final results before deciding what to write here and there.}
%
\item We perform a manual study and confirm that \tool test cases are accurately labelled  and categorized into \lcs. 
\end{itemize}