Review A
==========
- ***The paper did not tackle the problem of proposing new metamorphic 
  relations***. Instead, it assumed that the users already provided them 
  by means of user-specified linguistic capabilities, which work as 
  templates with parameters. Later, these templates and parameters can be 
  customized and updated with relevant words from some domain knowledge. 
  Generally, this approach is intuitive and naturally feasible.

- Compared to existing work, ***this approach works in finer granularity, 
  and thus can generate more similar test cases to cover more (corner) cases***.
  Therefore, the work itself has limited novelty, and this is my major concern.
  >The similar test cases can be generated by finding structural components in the context-free grammar for the expansion of each input sentence. However, another novelty of this work also comes from the way to parse each linguistic capability into the rules and predicates as it enables our approach to collect more diverse test cases that match the rules and predicates.

- First, some discussions lack neccessary details, in particular with the
  methodology part (Section 3). Overall, the methodology part needs more details 
  to be clarified, and explains how the oracle part in the generated (metamorphic) 
  test cases is prepared and validated before use.
  > Need to explain in more detail that the oracle is validated by only accepting neutral words and whether the expanded sentences match the corresponding linguistic capability as well as its seed sentences.

- Second, in Section 8, the related work part may miss some relevant work:

  S. Chen, S. Jin, X. Xie, Testing Your Question Answering Software via
  Asking Recursively, Proceedings of the 36th IEEE/ACM International
  Conference on Automated Software Engineering (ASE), 2021.

  S. Chen, S. Jin, X. Xie, Validation on machine reading comprehension
  software without annotated labels: a property-based method,
  Proceedings of the 29th ACM Joint Meeting on European Software
  Engineering Conference and Symposium on the Foundations of Software
  Engineering, 2021.


Review B
==========
- the details about linguistic capabilities are missing in both ALiCT 
  and the two baseline methods (CHECKLIST and HateCheck);
  in particular, ***how much manual work is needed to define ALiCT's
  generative rules, and are the same rules used in both AliCT and the
  baseline methods?*** Without such details, it is difficult to assess the comparisons and the real difference makers of ALiCT. ...
  
  ***The paper lacks important details about (manually) defining
  generative rules of linguistic capabilities and the linguistic capabilities used in the baseline methods*** (CHECKLIST and HateCheck in particular).
  > Need to say that the baselines do not use the rules, but only rely on descriptions of linguistic capabilities in natural language with manually generated test cases.

- Results (seed-passing but expansion-failing) reveal 
  the misclassification cases, without much insights offered into the bugs or their root causes.
  > Already mentioned at the application section.


Review C
==========
- Linguistic capabilities are defined as "a specific linguistic scenario 
  between input and output observed in the domain of NLP application", 
  but I found this unclear to understand the remainder of the introduction.

- For test case generation, "generative rules" and "enumerate predicates" 
  are mentioned as key terms, but not further explained.
  > Need to explain what the "generative rules" and "enumerate predicates" are.

- To tackle the test oracle problem, the paper claims to accept only 
  "validated test oracles defined by input-output relations determined by 
  the linguistic capability specification", but this also seemed ambiguous to me.
  > Need to explain how the validation relates to assign the test oracle.

- The section 3 starts without any additional motivation, 
  but introduces special terms for the phases and components.
  I would have found it very useful to explain the high-level ideas 
  and basic characteristics, such as ***the inputs and outputs of the system***.
  > What "additional motivation" we need? what is the "special" terms?

- Section 3.1, which explains the seed generation starts by explaining
  that ALiCT creates a generative rule, that two types of rules exist,
  and then how it enumerates rules. Neither of the steps was very clear to me.
  > I don't know what point is not clear. Maybe point of how the rules and predicates are generated.

- In the evaluation, I was surprised that Hatecheck, one of the two 
  baseline approaches, was not explained.
  